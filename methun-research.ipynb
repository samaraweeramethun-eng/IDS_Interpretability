{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-30T04:13:29.541170Z",
     "iopub.status.busy": "2025-08-30T04:13:29.540907Z",
     "iopub.status.idle": "2025-08-30T04:13:29.813617Z",
     "shell.execute_reply": "2025-08-30T04:13:29.812962Z",
     "shell.execute_reply.started": "2025-08-30T04:13:29.541149Z"
    },
    "papermill": {
     "duration": 1.87283,
     "end_time": "2025-07-13T17:12:24.057121",
     "exception": false,
     "start_time": "2025-07-13T17:12:22.184291",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/network-intrusion-dataset/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Monday-WorkingHours.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "/kaggle/input/network-intrusion-dataset/Wednesday-workingHours.pcap_ISCX.csv\n",
      "/kaggle/input/cicids2017/cicids2017.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T04:13:29.849762Z",
     "iopub.status.busy": "2025-08-30T04:13:29.849481Z",
     "iopub.status.idle": "2025-08-30T04:51:03.780263Z",
     "shell.execute_reply": "2025-08-30T04:51:03.779576Z",
     "shell.execute_reply.started": "2025-08-30T04:13:29.849746Z"
    },
    "papermill": {
     "duration": 2281.454616,
     "end_time": "2025-07-13T17:50:25.514282",
     "exception": false,
     "start_time": "2025-07-13T17:12:24.059666",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CUDA available with 2 GPU(s)\n",
      "   GPU 0: Tesla T4 (14.7GB)\n",
      "   GPU 1: Tesla T4 (14.7GB)\n",
      "üöÄ Enhanced Binary RTIDS Training - GPU T4 x2 Optimized (No Synthetic Samples, No Feature Engineering, No Early Stopping)\n",
      "üéØ Target: 99.85%+ ROC-AUC with optimal hyperparameters\n",
      "‚úÖ Full epoch training - NO synthetic samples, NO feature engineering, NO early stopping\n",
      "====================================================================================================\n",
      "üìä Loading CICIDS2017 dataset...\n",
      "Dataset shape: (2830743, 79)\n",
      "‚úÖ Found label column: Label\n",
      "üîÑ Converting to binary classification...\n",
      "Binary distribution:\n",
      "Normal (0): 2,273,097\n",
      "Attack (1): 557,646\n",
      "üìä Using original features only...\n",
      "Using 78 original features (no feature engineering)\n",
      "üßπ Cleaning data...\n",
      "üìè Advanced preprocessing...\n",
      "üîß Advanced preprocessing...\n",
      "‚úÖ Outlier capping applied\n",
      "‚úÖ Applied quantile scaling\n",
      "‚öñÔ∏è Intelligent class balancing (undersampling only)...\n",
      "‚öñÔ∏è Intelligent class balancing (undersampling only)...\n",
      "Original distribution: {'Normal': 2273097, 'Attack': 557646}\n",
      "‚úÖ Applied distance-based intelligent undersampling\n",
      "Final distribution: Normal=1,672,938, Attack=557,646\n",
      "Class ratio: 3.00:1 (Normal:Attack)\n",
      "‚úÇÔ∏è Splitting data...\n",
      "Training set: 1,784,467 samples\n",
      "Validation set: 446,117 samples\n",
      "Features: 78\n",
      "üìä Class weights: Normal=0.667, Attack=2.000\n",
      "ü§ñ Creating Enhanced Binary Transformer...\n",
      "üî• Using DataParallel with 2 GPUs\n",
      "üíª Using device: cuda:0\n",
      "üî• Multi-GPU mode enabled\n",
      "üìà Total parameters: 1,393,084\n",
      "üìà Trainable parameters: 1,393,084\n",
      "üîÑ Creating enhanced data loaders...\n",
      "üìä Dataset sizes - Train: 1784467, Val: 446117\n",
      "üéØ Enhanced Loss: Adaptive Focal Loss (Œ≥=learnable, Œ±=0.75)\n",
      "üéØ Optimizer: AdamW (lr=0.002)\n",
      "üéØ Advanced Features: SWA=True, Mixup=True\n",
      "üéØ Training: Full 35 epochs (no early stopping)\n",
      "üéØ Data Strategy: Original features + intelligent undersampling only (NO synthetic samples, NO feature engineering)\n",
      "\n",
      "üöÄ Starting enhanced training...\n",
      "================================================================================\n",
      "Epoch  1 | Batch    0 | Loss: 0.2485 | LR: 8.00e-05 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  100 | Loss: 0.0305 | LR: 8.13e-05 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  200 | Loss: 0.0146 | LR: 8.51e-05 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  300 | Loss: 0.0130 | LR: 9.15e-05 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  400 | Loss: 0.0119 | LR: 1.00e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  500 | Loss: 0.0139 | LR: 1.12e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  600 | Loss: 0.0103 | LR: 1.26e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  700 | Loss: 0.0121 | LR: 1.42e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  800 | Loss: 0.0099 | LR: 1.61e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch  900 | Loss: 0.0099 | LR: 1.82e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1000 | Loss: 0.0098 | LR: 2.05e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1100 | Loss: 0.0087 | LR: 2.30e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1200 | Loss: 0.0100 | LR: 2.58e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1300 | Loss: 0.0083 | LR: 2.88e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1400 | Loss: 0.0091 | LR: 3.20e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1500 | Loss: 0.0099 | LR: 3.53e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1600 | Loss: 0.0086 | LR: 3.89e-04 | Œ≥: 1.800\n",
      "Epoch  1 | Batch 1700 | Loss: 0.0088 | LR: 4.26e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch  1 Summary:\n",
      "  Train Loss: 0.0140 | Val Loss: 0.0097\n",
      "  Accuracy: 0.9611 | ROC-AUC: 0.9994 | PR-AUC: 0.9970\n",
      "  F1: 0.9278 | Precision: 0.8655 | Recall: 0.9997\n",
      "  üíæ New best model saved! (AUC: 0.999398)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  2 | Batch    0 | Loss: 0.0098 | LR: 4.42e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  100 | Loss: 0.0074 | LR: 4.81e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  200 | Loss: 0.0108 | LR: 5.22e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  300 | Loss: 0.0090 | LR: 5.65e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  400 | Loss: 0.0082 | LR: 6.08e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  500 | Loss: 0.0073 | LR: 6.53e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  600 | Loss: 0.0077 | LR: 6.99e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  700 | Loss: 0.0097 | LR: 7.46e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  800 | Loss: 0.0079 | LR: 7.93e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch  900 | Loss: 0.0071 | LR: 8.41e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1000 | Loss: 0.0082 | LR: 8.90e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1100 | Loss: 0.0086 | LR: 9.39e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1200 | Loss: 0.0083 | LR: 9.88e-04 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1300 | Loss: 0.0076 | LR: 1.04e-03 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1400 | Loss: 0.0068 | LR: 1.09e-03 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1500 | Loss: 0.0103 | LR: 1.14e-03 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1600 | Loss: 0.0085 | LR: 1.19e-03 | Œ≥: 1.800\n",
      "Epoch  2 | Batch 1700 | Loss: 0.0078 | LR: 1.23e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  2 Summary:\n",
      "  Train Loss: 0.0086 | Val Loss: 0.0086\n",
      "  Accuracy: 0.9711 | ROC-AUC: 0.9998 | PR-AUC: 0.9993\n",
      "  F1: 0.9454 | Precision: 0.8966 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999765)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  3 | Batch    0 | Loss: 0.0095 | LR: 1.25e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  100 | Loss: 0.0076 | LR: 1.30e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  200 | Loss: 0.0071 | LR: 1.35e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  300 | Loss: 0.0085 | LR: 1.40e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  400 | Loss: 0.0078 | LR: 1.44e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  500 | Loss: 0.0081 | LR: 1.49e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  600 | Loss: 0.0083 | LR: 1.53e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  700 | Loss: 0.0069 | LR: 1.57e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  800 | Loss: 0.0078 | LR: 1.61e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch  900 | Loss: 0.0066 | LR: 1.65e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1000 | Loss: 0.0090 | LR: 1.69e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1100 | Loss: 0.0068 | LR: 1.72e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1200 | Loss: 0.0099 | LR: 1.76e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1300 | Loss: 0.0116 | LR: 1.79e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1400 | Loss: 0.0069 | LR: 1.82e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1500 | Loss: 0.0089 | LR: 1.85e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1600 | Loss: 0.0092 | LR: 1.87e-03 | Œ≥: 1.800\n",
      "Epoch  3 | Batch 1700 | Loss: 0.0065 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  3 Summary:\n",
      "  Train Loss: 0.0084 | Val Loss: 0.0080\n",
      "  Accuracy: 0.9716 | ROC-AUC: 0.9998 | PR-AUC: 0.9994\n",
      "  F1: 0.9462 | Precision: 0.8981 | Recall: 0.9999\n",
      "  üíæ New best model saved! (AUC: 0.999807)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  4 | Batch    0 | Loss: 0.0089 | LR: 1.91e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  100 | Loss: 0.0061 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  200 | Loss: 0.0264 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  300 | Loss: 0.0112 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  400 | Loss: 0.0092 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  500 | Loss: 0.0076 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  600 | Loss: 0.0090 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  700 | Loss: 0.0090 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  800 | Loss: 0.0076 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch  900 | Loss: 0.0095 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1000 | Loss: 0.0077 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1100 | Loss: 0.0075 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1200 | Loss: 0.0078 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1300 | Loss: 0.0076 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1400 | Loss: 0.0096 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1500 | Loss: 0.0080 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1600 | Loss: 0.0086 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  4 | Batch 1700 | Loss: 0.0067 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  4 Summary:\n",
      "  Train Loss: 0.0091 | Val Loss: 0.0084\n",
      "  Accuracy: 0.9709 | ROC-AUC: 0.9997 | PR-AUC: 0.9991\n",
      "  F1: 0.9450 | Precision: 0.8958 | Recall: 0.9999\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  5 | Batch    0 | Loss: 0.0073 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  100 | Loss: 0.0068 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  200 | Loss: 0.0073 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  300 | Loss: 0.0080 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  400 | Loss: 0.0084 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  500 | Loss: 0.0090 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  600 | Loss: 0.0069 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  700 | Loss: 0.0064 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  800 | Loss: 0.0083 | LR: 2.00e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch  900 | Loss: 0.0083 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1000 | Loss: 0.0098 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1100 | Loss: 0.0067 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1200 | Loss: 0.0094 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1300 | Loss: 0.0071 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1400 | Loss: 0.0067 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1500 | Loss: 0.0076 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1600 | Loss: 0.0068 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  5 | Batch 1700 | Loss: 0.0080 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  5 Summary:\n",
      "  Train Loss: 0.0079 | Val Loss: 0.0079\n",
      "  Accuracy: 0.9808 | ROC-AUC: 0.9998 | PR-AUC: 0.9994\n",
      "  F1: 0.9631 | Precision: 0.9291 | Recall: 0.9997\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  6 | Batch    0 | Loss: 0.0075 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  100 | Loss: 0.0074 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  200 | Loss: 0.0069 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  300 | Loss: 0.0077 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  400 | Loss: 0.0068 | LR: 1.99e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  500 | Loss: 0.0121 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  600 | Loss: 0.0085 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  700 | Loss: 0.0123 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  800 | Loss: 0.0076 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch  900 | Loss: 0.0075 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1000 | Loss: 0.0088 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1100 | Loss: 0.0085 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1200 | Loss: 0.0080 | LR: 1.98e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1300 | Loss: 0.0081 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1400 | Loss: 0.0078 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1500 | Loss: 0.0095 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1600 | Loss: 0.0075 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  6 | Batch 1700 | Loss: 0.0073 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  6 Summary:\n",
      "  Train Loss: 0.0082 | Val Loss: 0.0077\n",
      "  Accuracy: 0.9869 | ROC-AUC: 0.9998 | PR-AUC: 0.9995\n",
      "  F1: 0.9744 | Precision: 0.9506 | Recall: 0.9994\n",
      "  üíæ New best model saved! (AUC: 0.999821)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  7 | Batch    0 | Loss: 0.0071 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  100 | Loss: 0.0229 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  200 | Loss: 0.0612 | LR: 1.97e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  300 | Loss: 0.0414 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  400 | Loss: 0.0158 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  500 | Loss: 0.0223 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  600 | Loss: 0.0136 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  700 | Loss: 0.0116 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  800 | Loss: 0.0113 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch  900 | Loss: 0.0151 | LR: 1.96e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1000 | Loss: 0.0269 | LR: 1.95e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1100 | Loss: 0.0109 | LR: 1.95e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1200 | Loss: 0.0106 | LR: 1.95e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1300 | Loss: 0.0145 | LR: 1.95e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1400 | Loss: 0.0124 | LR: 1.95e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1500 | Loss: 0.0245 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1600 | Loss: 0.0111 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  7 | Batch 1700 | Loss: 0.0580 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  7 Summary:\n",
      "  Train Loss: 0.0329 | Val Loss: 0.0113\n",
      "  Accuracy: 0.9250 | ROC-AUC: 0.9997 | PR-AUC: 0.9991\n",
      "  F1: 0.8696 | Precision: 0.7693 | Recall: 0.9999\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  8 | Batch    0 | Loss: 0.0178 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  100 | Loss: 0.0095 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  200 | Loss: 0.0520 | LR: 1.94e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  300 | Loss: 0.0544 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  400 | Loss: 0.0104 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  500 | Loss: 0.0340 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  600 | Loss: 0.0108 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  700 | Loss: 0.0530 | LR: 1.93e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  800 | Loss: 0.0089 | LR: 1.92e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch  900 | Loss: 0.0105 | LR: 1.92e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1000 | Loss: 0.0097 | LR: 1.92e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1100 | Loss: 0.0102 | LR: 1.92e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1200 | Loss: 0.0083 | LR: 1.91e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1300 | Loss: 0.0532 | LR: 1.91e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1400 | Loss: 0.0553 | LR: 1.91e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1500 | Loss: 0.0543 | LR: 1.91e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1600 | Loss: 0.0272 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "Epoch  8 | Batch 1700 | Loss: 0.0565 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  8 Summary:\n",
      "  Train Loss: 0.0279 | Val Loss: 0.0089\n",
      "  Accuracy: 0.9690 | ROC-AUC: 0.9998 | PR-AUC: 0.9994\n",
      "  F1: 0.9415 | Precision: 0.8897 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  9 | Batch    0 | Loss: 0.0546 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  100 | Loss: 0.0541 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  200 | Loss: 0.0135 | LR: 1.90e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  300 | Loss: 0.0091 | LR: 1.89e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  400 | Loss: 0.0556 | LR: 1.89e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  500 | Loss: 0.0303 | LR: 1.89e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  600 | Loss: 0.0138 | LR: 1.89e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  700 | Loss: 0.0104 | LR: 1.88e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  800 | Loss: 0.0105 | LR: 1.88e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch  900 | Loss: 0.0090 | LR: 1.88e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1000 | Loss: 0.0219 | LR: 1.87e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1100 | Loss: 0.0440 | LR: 1.87e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1200 | Loss: 0.0382 | LR: 1.87e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1300 | Loss: 0.0146 | LR: 1.87e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1400 | Loss: 0.0413 | LR: 1.86e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1500 | Loss: 0.0081 | LR: 1.86e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1600 | Loss: 0.0549 | LR: 1.86e-03 | Œ≥: 1.800\n",
      "Epoch  9 | Batch 1700 | Loss: 0.0546 | LR: 1.85e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch  9 Summary:\n",
      "  Train Loss: 0.0272 | Val Loss: 0.0078\n",
      "  Accuracy: 0.9863 | ROC-AUC: 0.9997 | PR-AUC: 0.9984\n",
      "  F1: 0.9734 | Precision: 0.9485 | Recall: 0.9997\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 | Batch    0 | Loss: 0.0499 | LR: 1.85e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  100 | Loss: 0.0094 | LR: 1.85e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  200 | Loss: 0.0109 | LR: 1.85e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  300 | Loss: 0.0080 | LR: 1.84e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  400 | Loss: 0.0570 | LR: 1.84e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  500 | Loss: 0.0118 | LR: 1.84e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  600 | Loss: 0.0519 | LR: 1.83e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  700 | Loss: 0.0434 | LR: 1.83e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  800 | Loss: 0.0506 | LR: 1.83e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch  900 | Loss: 0.0486 | LR: 1.83e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1000 | Loss: 0.0407 | LR: 1.82e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1100 | Loss: 0.0544 | LR: 1.82e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1200 | Loss: 0.0240 | LR: 1.82e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1300 | Loss: 0.0081 | LR: 1.81e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1400 | Loss: 0.0084 | LR: 1.81e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1500 | Loss: 0.0109 | LR: 1.81e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1600 | Loss: 0.0470 | LR: 1.80e-03 | Œ≥: 1.800\n",
      "Epoch 10 | Batch 1700 | Loss: 0.0094 | LR: 1.80e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 10 Summary:\n",
      "  Train Loss: 0.0258 | Val Loss: 0.0078\n",
      "  Accuracy: 0.9863 | ROC-AUC: 0.9998 | PR-AUC: 0.9991\n",
      "  F1: 0.9734 | Precision: 0.9483 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 | Batch    0 | Loss: 0.0369 | LR: 1.80e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  100 | Loss: 0.0102 | LR: 1.79e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  200 | Loss: 0.0435 | LR: 1.79e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  300 | Loss: 0.0090 | LR: 1.79e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  400 | Loss: 0.0111 | LR: 1.78e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  500 | Loss: 0.0076 | LR: 1.78e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  600 | Loss: 0.0080 | LR: 1.78e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  700 | Loss: 0.0178 | LR: 1.77e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  800 | Loss: 0.0248 | LR: 1.77e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch  900 | Loss: 0.0477 | LR: 1.76e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1000 | Loss: 0.0073 | LR: 1.76e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1100 | Loss: 0.0094 | LR: 1.76e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1200 | Loss: 0.0343 | LR: 1.75e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1300 | Loss: 0.0240 | LR: 1.75e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1400 | Loss: 0.0450 | LR: 1.75e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1500 | Loss: 0.0093 | LR: 1.74e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1600 | Loss: 0.0381 | LR: 1.74e-03 | Œ≥: 1.800\n",
      "Epoch 11 | Batch 1700 | Loss: 0.0417 | LR: 1.73e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 11 Summary:\n",
      "  Train Loss: 0.0261 | Val Loss: 0.0074\n",
      "  Accuracy: 0.9909 | ROC-AUC: 0.9997 | PR-AUC: 0.9988\n",
      "  F1: 0.9820 | Precision: 0.9651 | Recall: 0.9996\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 | Batch    0 | Loss: 0.0527 | LR: 1.73e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  100 | Loss: 0.0501 | LR: 1.73e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  200 | Loss: 0.0535 | LR: 1.73e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  300 | Loss: 0.0408 | LR: 1.72e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  400 | Loss: 0.0085 | LR: 1.72e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  500 | Loss: 0.0101 | LR: 1.71e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  600 | Loss: 0.0075 | LR: 1.71e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  700 | Loss: 0.0467 | LR: 1.71e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  800 | Loss: 0.0615 | LR: 1.70e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch  900 | Loss: 0.0126 | LR: 1.70e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1000 | Loss: 0.0102 | LR: 1.69e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1100 | Loss: 0.0073 | LR: 1.69e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1200 | Loss: 0.0399 | LR: 1.68e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1300 | Loss: 0.0086 | LR: 1.68e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1400 | Loss: 0.0240 | LR: 1.68e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1500 | Loss: 0.0367 | LR: 1.67e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1600 | Loss: 0.0096 | LR: 1.67e-03 | Œ≥: 1.800\n",
      "Epoch 12 | Batch 1700 | Loss: 0.0075 | LR: 1.66e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 12 Summary:\n",
      "  Train Loss: 0.0254 | Val Loss: 0.0076\n",
      "  Accuracy: 0.9897 | ROC-AUC: 0.9998 | PR-AUC: 0.9995\n",
      "  F1: 0.9797 | Precision: 0.9607 | Recall: 0.9995\n",
      "  üíæ New best model saved! (AUC: 0.999846)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 | Batch    0 | Loss: 0.0082 | LR: 1.66e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  100 | Loss: 0.0228 | LR: 1.66e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  200 | Loss: 0.0131 | LR: 1.65e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  300 | Loss: 0.0226 | LR: 1.65e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  400 | Loss: 0.0084 | LR: 1.64e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  500 | Loss: 0.0527 | LR: 1.64e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  600 | Loss: 0.0522 | LR: 1.64e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  700 | Loss: 0.0512 | LR: 1.63e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  800 | Loss: 0.0088 | LR: 1.63e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch  900 | Loss: 0.0529 | LR: 1.62e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1000 | Loss: 0.0087 | LR: 1.62e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1100 | Loss: 0.0270 | LR: 1.61e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1200 | Loss: 0.0183 | LR: 1.61e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1300 | Loss: 0.0116 | LR: 1.60e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1400 | Loss: 0.0146 | LR: 1.60e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1500 | Loss: 0.0077 | LR: 1.59e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1600 | Loss: 0.0416 | LR: 1.59e-03 | Œ≥: 1.800\n",
      "Epoch 13 | Batch 1700 | Loss: 0.0290 | LR: 1.59e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 13 Summary:\n",
      "  Train Loss: 0.0249 | Val Loss: 0.0081\n",
      "  Accuracy: 0.9898 | ROC-AUC: 0.9998 | PR-AUC: 0.9992\n",
      "  F1: 0.9800 | Precision: 0.9610 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 | Batch    0 | Loss: 0.0380 | LR: 1.58e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  100 | Loss: 0.0173 | LR: 1.58e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  200 | Loss: 0.0524 | LR: 1.57e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  300 | Loss: 0.0528 | LR: 1.57e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  400 | Loss: 0.0511 | LR: 1.56e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  500 | Loss: 0.0077 | LR: 1.56e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  600 | Loss: 0.0090 | LR: 1.56e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  700 | Loss: 0.0095 | LR: 1.55e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  800 | Loss: 0.0064 | LR: 1.55e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch  900 | Loss: 0.0067 | LR: 1.54e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1000 | Loss: 0.0307 | LR: 1.54e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1100 | Loss: 0.0098 | LR: 1.53e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1200 | Loss: 0.0167 | LR: 1.53e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1300 | Loss: 0.0433 | LR: 1.52e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1400 | Loss: 0.0521 | LR: 1.52e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1500 | Loss: 0.0086 | LR: 1.51e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1600 | Loss: 0.0411 | LR: 1.51e-03 | Œ≥: 1.800\n",
      "Epoch 14 | Batch 1700 | Loss: 0.0464 | LR: 1.50e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 14 Summary:\n",
      "  Train Loss: 0.0254 | Val Loss: 0.0076\n",
      "  Accuracy: 0.9932 | ROC-AUC: 0.9997 | PR-AUC: 0.9993\n",
      "  F1: 0.9866 | Precision: 0.9748 | Recall: 0.9986\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 | Batch    0 | Loss: 0.0067 | LR: 1.50e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  100 | Loss: 0.0248 | LR: 1.49e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  200 | Loss: 0.0071 | LR: 1.49e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  300 | Loss: 0.0080 | LR: 1.48e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  400 | Loss: 0.0072 | LR: 1.48e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  500 | Loss: 0.0197 | LR: 1.47e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  600 | Loss: 0.0543 | LR: 1.47e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  700 | Loss: 0.0497 | LR: 1.46e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  800 | Loss: 0.0193 | LR: 1.46e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch  900 | Loss: 0.0118 | LR: 1.45e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1000 | Loss: 0.0072 | LR: 1.45e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1100 | Loss: 0.0149 | LR: 1.44e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1200 | Loss: 0.0114 | LR: 1.44e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1300 | Loss: 0.0479 | LR: 1.43e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1400 | Loss: 0.0072 | LR: 1.43e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1500 | Loss: 0.0073 | LR: 1.42e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1600 | Loss: 0.0299 | LR: 1.42e-03 | Œ≥: 1.800\n",
      "Epoch 15 | Batch 1700 | Loss: 0.0469 | LR: 1.41e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 15 Summary:\n",
      "  Train Loss: 0.0249 | Val Loss: 0.0076\n",
      "  Accuracy: 0.9922 | ROC-AUC: 0.9999 | PR-AUC: 0.9996\n",
      "  F1: 0.9845 | Precision: 0.9699 | Recall: 0.9997\n",
      "  üíæ New best model saved! (AUC: 0.999868)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 | Batch    0 | Loss: 0.0210 | LR: 1.41e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  100 | Loss: 0.0077 | LR: 1.41e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  200 | Loss: 0.0199 | LR: 1.40e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  300 | Loss: 0.0064 | LR: 1.40e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  400 | Loss: 0.0373 | LR: 1.39e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  500 | Loss: 0.0228 | LR: 1.38e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  600 | Loss: 0.0069 | LR: 1.38e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  700 | Loss: 0.0075 | LR: 1.37e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  800 | Loss: 0.0486 | LR: 1.37e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch  900 | Loss: 0.0078 | LR: 1.36e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1000 | Loss: 0.0074 | LR: 1.36e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1100 | Loss: 0.0405 | LR: 1.35e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1200 | Loss: 0.0077 | LR: 1.35e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1300 | Loss: 0.0211 | LR: 1.34e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1400 | Loss: 0.0242 | LR: 1.34e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1500 | Loss: 0.0397 | LR: 1.33e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1600 | Loss: 0.0279 | LR: 1.33e-03 | Œ≥: 1.800\n",
      "Epoch 16 | Batch 1700 | Loss: 0.0100 | LR: 1.32e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 16 Summary:\n",
      "  Train Loss: 0.0256 | Val Loss: 0.0078\n",
      "  Accuracy: 0.9854 | ROC-AUC: 0.9999 | PR-AUC: 0.9996\n",
      "  F1: 0.9717 | Precision: 0.9452 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999872)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 | Batch    0 | Loss: 0.0456 | LR: 1.32e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  100 | Loss: 0.0251 | LR: 1.31e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  200 | Loss: 0.0281 | LR: 1.31e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  300 | Loss: 0.0158 | LR: 1.30e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  400 | Loss: 0.0213 | LR: 1.30e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  500 | Loss: 0.0148 | LR: 1.29e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  600 | Loss: 0.0373 | LR: 1.29e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  700 | Loss: 0.0344 | LR: 1.28e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  800 | Loss: 0.0080 | LR: 1.27e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch  900 | Loss: 0.0234 | LR: 1.27e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1000 | Loss: 0.0276 | LR: 1.26e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1100 | Loss: 0.0071 | LR: 1.26e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1200 | Loss: 0.0348 | LR: 1.25e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1300 | Loss: 0.0094 | LR: 1.25e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1400 | Loss: 0.0412 | LR: 1.24e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1500 | Loss: 0.0485 | LR: 1.24e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1600 | Loss: 0.0093 | LR: 1.23e-03 | Œ≥: 1.800\n",
      "Epoch 17 | Batch 1700 | Loss: 0.0078 | LR: 1.22e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 17 Summary:\n",
      "  Train Loss: 0.0243 | Val Loss: 0.0072\n",
      "  Accuracy: 0.9932 | ROC-AUC: 0.9999 | PR-AUC: 0.9996\n",
      "  F1: 0.9866 | Precision: 0.9738 | Recall: 0.9997\n",
      "  üíæ New best model saved! (AUC: 0.999885)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 | Batch    0 | Loss: 0.0512 | LR: 1.22e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  100 | Loss: 0.0069 | LR: 1.22e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  200 | Loss: 0.0441 | LR: 1.21e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  300 | Loss: 0.0066 | LR: 1.21e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  400 | Loss: 0.0083 | LR: 1.20e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  500 | Loss: 0.0079 | LR: 1.19e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  600 | Loss: 0.0071 | LR: 1.19e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  700 | Loss: 0.0071 | LR: 1.18e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  800 | Loss: 0.0212 | LR: 1.18e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch  900 | Loss: 0.0484 | LR: 1.17e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1000 | Loss: 0.0458 | LR: 1.17e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1100 | Loss: 0.0081 | LR: 1.16e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1200 | Loss: 0.0518 | LR: 1.15e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1300 | Loss: 0.0258 | LR: 1.15e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1400 | Loss: 0.0159 | LR: 1.14e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1500 | Loss: 0.0275 | LR: 1.14e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1600 | Loss: 0.0077 | LR: 1.13e-03 | Œ≥: 1.800\n",
      "Epoch 18 | Batch 1700 | Loss: 0.0511 | LR: 1.13e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 18 Summary:\n",
      "  Train Loss: 0.0245 | Val Loss: 0.0072\n",
      "  Accuracy: 0.9931 | ROC-AUC: 0.9998 | PR-AUC: 0.9993\n",
      "  F1: 0.9864 | Precision: 0.9733 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 | Batch    0 | Loss: 0.0067 | LR: 1.12e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  100 | Loss: 0.0255 | LR: 1.12e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  200 | Loss: 0.0284 | LR: 1.11e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  300 | Loss: 0.0074 | LR: 1.11e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  400 | Loss: 0.0071 | LR: 1.10e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  500 | Loss: 0.0096 | LR: 1.10e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  600 | Loss: 0.0449 | LR: 1.09e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  700 | Loss: 0.0383 | LR: 1.08e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  800 | Loss: 0.0157 | LR: 1.08e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch  900 | Loss: 0.0066 | LR: 1.07e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1000 | Loss: 0.0274 | LR: 1.07e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1100 | Loss: 0.0070 | LR: 1.06e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1200 | Loss: 0.0488 | LR: 1.06e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1300 | Loss: 0.0087 | LR: 1.05e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1400 | Loss: 0.0491 | LR: 1.04e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1500 | Loss: 0.0455 | LR: 1.04e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1600 | Loss: 0.0516 | LR: 1.03e-03 | Œ≥: 1.800\n",
      "Epoch 19 | Batch 1700 | Loss: 0.0396 | LR: 1.03e-03 | Œ≥: 1.800\n",
      "\n",
      "Epoch 19 Summary:\n",
      "  Train Loss: 0.0241 | Val Loss: 0.0071\n",
      "  Accuracy: 0.9939 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9879 | Precision: 0.9764 | Recall: 0.9997\n",
      "  üíæ New best model saved! (AUC: 0.999898)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20 | Batch    0 | Loss: 0.0385 | LR: 1.02e-03 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  100 | Loss: 0.0485 | LR: 1.02e-03 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  200 | Loss: 0.0240 | LR: 1.01e-03 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  300 | Loss: 0.0491 | LR: 1.01e-03 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  400 | Loss: 0.0251 | LR: 1.00e-03 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  500 | Loss: 0.0085 | LR: 9.96e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  600 | Loss: 0.0106 | LR: 9.90e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  700 | Loss: 0.0174 | LR: 9.85e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  800 | Loss: 0.0109 | LR: 9.79e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch  900 | Loss: 0.0083 | LR: 9.73e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1000 | Loss: 0.0345 | LR: 9.68e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1100 | Loss: 0.0112 | LR: 9.62e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1200 | Loss: 0.0517 | LR: 9.56e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1300 | Loss: 0.0183 | LR: 9.50e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1400 | Loss: 0.0088 | LR: 9.45e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1500 | Loss: 0.0143 | LR: 9.39e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1600 | Loss: 0.0073 | LR: 9.33e-04 | Œ≥: 1.800\n",
      "Epoch 20 | Batch 1700 | Loss: 0.0470 | LR: 9.28e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 20 Summary:\n",
      "  Train Loss: 0.0234 | Val Loss: 0.0074\n",
      "  Accuracy: 0.9939 | ROC-AUC: 0.9998 | PR-AUC: 0.9992\n",
      "  F1: 0.9879 | Precision: 0.9763 | Recall: 0.9997\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 21 | Batch    0 | Loss: 0.0444 | LR: 9.25e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  100 | Loss: 0.0071 | LR: 9.19e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  200 | Loss: 0.0080 | LR: 9.14e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  300 | Loss: 0.0230 | LR: 9.08e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  400 | Loss: 0.0070 | LR: 9.02e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  500 | Loss: 0.0525 | LR: 8.97e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  600 | Loss: 0.0077 | LR: 8.91e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  700 | Loss: 0.0074 | LR: 8.85e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  800 | Loss: 0.0129 | LR: 8.80e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch  900 | Loss: 0.0066 | LR: 8.74e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1000 | Loss: 0.0504 | LR: 8.68e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1100 | Loss: 0.0483 | LR: 8.63e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1200 | Loss: 0.0142 | LR: 8.57e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1300 | Loss: 0.0083 | LR: 8.51e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1400 | Loss: 0.0372 | LR: 8.46e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1500 | Loss: 0.0370 | LR: 8.40e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1600 | Loss: 0.0119 | LR: 8.34e-04 | Œ≥: 1.800\n",
      "Epoch 21 | Batch 1700 | Loss: 0.0085 | LR: 8.29e-04 | Œ≥: 1.800\n",
      "üìä SWA model updated (n_averaged: 1)\n",
      "\n",
      "Epoch 21 Summary:\n",
      "  Train Loss: 0.0238 | Val Loss: 0.0070\n",
      "  Accuracy: 0.9937 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9876 | Precision: 0.9756 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999905)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 22 | Batch    0 | Loss: 0.0176 | LR: 8.26e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  100 | Loss: 0.0079 | LR: 8.21e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  200 | Loss: 0.0067 | LR: 8.15e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  300 | Loss: 0.0121 | LR: 8.09e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  400 | Loss: 0.0156 | LR: 8.04e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  500 | Loss: 0.0069 | LR: 7.98e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  600 | Loss: 0.0069 | LR: 7.93e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  700 | Loss: 0.0538 | LR: 7.87e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  800 | Loss: 0.0333 | LR: 7.81e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch  900 | Loss: 0.0478 | LR: 7.76e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1000 | Loss: 0.0365 | LR: 7.70e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1100 | Loss: 0.0448 | LR: 7.65e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1200 | Loss: 0.0478 | LR: 7.59e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1300 | Loss: 0.0067 | LR: 7.53e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1400 | Loss: 0.0519 | LR: 7.48e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1500 | Loss: 0.0068 | LR: 7.42e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1600 | Loss: 0.0485 | LR: 7.37e-04 | Œ≥: 1.800\n",
      "Epoch 22 | Batch 1700 | Loss: 0.0214 | LR: 7.31e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 22 Summary:\n",
      "  Train Loss: 0.0235 | Val Loss: 0.0069\n",
      "  Accuracy: 0.9952 | ROC-AUC: 0.9999 | PR-AUC: 0.9996\n",
      "  F1: 0.9906 | Precision: 0.9815 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 23 | Batch    0 | Loss: 0.0118 | LR: 7.29e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  100 | Loss: 0.0453 | LR: 7.24e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  200 | Loss: 0.0351 | LR: 7.18e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  300 | Loss: 0.0266 | LR: 7.13e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  400 | Loss: 0.0402 | LR: 7.07e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  500 | Loss: 0.0527 | LR: 7.02e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  600 | Loss: 0.0069 | LR: 6.96e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  700 | Loss: 0.0064 | LR: 6.91e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  800 | Loss: 0.0539 | LR: 6.85e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch  900 | Loss: 0.0062 | LR: 6.80e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1000 | Loss: 0.0063 | LR: 6.74e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1100 | Loss: 0.0072 | LR: 6.69e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1200 | Loss: 0.0534 | LR: 6.64e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1300 | Loss: 0.0298 | LR: 6.58e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1400 | Loss: 0.0421 | LR: 6.53e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1500 | Loss: 0.0114 | LR: 6.47e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1600 | Loss: 0.0070 | LR: 6.42e-04 | Œ≥: 1.800\n",
      "Epoch 23 | Batch 1700 | Loss: 0.0478 | LR: 6.37e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 23 Summary:\n",
      "  Train Loss: 0.0239 | Val Loss: 0.0069\n",
      "  Accuracy: 0.9922 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9846 | Precision: 0.9699 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 24 | Batch    0 | Loss: 0.0068 | LR: 6.35e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  100 | Loss: 0.0215 | LR: 6.29e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  200 | Loss: 0.0063 | LR: 6.24e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  300 | Loss: 0.0074 | LR: 6.19e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  400 | Loss: 0.0088 | LR: 6.13e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  500 | Loss: 0.0519 | LR: 6.08e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  600 | Loss: 0.0187 | LR: 6.03e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  700 | Loss: 0.0118 | LR: 5.98e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  800 | Loss: 0.0492 | LR: 5.92e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch  900 | Loss: 0.0249 | LR: 5.87e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1000 | Loss: 0.0237 | LR: 5.82e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1100 | Loss: 0.0082 | LR: 5.77e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1200 | Loss: 0.0068 | LR: 5.72e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1300 | Loss: 0.0512 | LR: 5.66e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1400 | Loss: 0.0105 | LR: 5.61e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1500 | Loss: 0.0523 | LR: 5.56e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1600 | Loss: 0.0429 | LR: 5.51e-04 | Œ≥: 1.800\n",
      "Epoch 24 | Batch 1700 | Loss: 0.0161 | LR: 5.46e-04 | Œ≥: 1.800\n",
      "üìä SWA model updated (n_averaged: 2)\n",
      "\n",
      "Epoch 24 Summary:\n",
      "  Train Loss: 0.0236 | Val Loss: 0.0071\n",
      "  Accuracy: 0.9938 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9877 | Precision: 0.9759 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999915)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25 | Batch    0 | Loss: 0.0103 | LR: 5.44e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  100 | Loss: 0.0502 | LR: 5.39e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  200 | Loss: 0.0121 | LR: 5.34e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  300 | Loss: 0.0070 | LR: 5.28e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  400 | Loss: 0.0070 | LR: 5.23e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  500 | Loss: 0.0516 | LR: 5.18e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  600 | Loss: 0.0071 | LR: 5.13e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  700 | Loss: 0.0083 | LR: 5.08e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  800 | Loss: 0.0064 | LR: 5.03e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch  900 | Loss: 0.0276 | LR: 4.98e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1000 | Loss: 0.0063 | LR: 4.94e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1100 | Loss: 0.0077 | LR: 4.89e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1200 | Loss: 0.0409 | LR: 4.84e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1300 | Loss: 0.0403 | LR: 4.79e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1400 | Loss: 0.0383 | LR: 4.74e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1500 | Loss: 0.0123 | LR: 4.69e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1600 | Loss: 0.0389 | LR: 4.64e-04 | Œ≥: 1.800\n",
      "Epoch 25 | Batch 1700 | Loss: 0.0470 | LR: 4.59e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 25 Summary:\n",
      "  Train Loss: 0.0225 | Val Loss: 0.0069\n",
      "  Accuracy: 0.9950 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9901 | Precision: 0.9805 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 26 | Batch    0 | Loss: 0.0511 | LR: 4.57e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  100 | Loss: 0.0062 | LR: 4.53e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  200 | Loss: 0.0355 | LR: 4.48e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  300 | Loss: 0.0094 | LR: 4.43e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  400 | Loss: 0.0078 | LR: 4.38e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  500 | Loss: 0.0474 | LR: 4.34e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  600 | Loss: 0.0152 | LR: 4.29e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  700 | Loss: 0.0060 | LR: 4.24e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  800 | Loss: 0.0311 | LR: 4.19e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch  900 | Loss: 0.0440 | LR: 4.15e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1000 | Loss: 0.0492 | LR: 4.10e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1100 | Loss: 0.0140 | LR: 4.06e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1200 | Loss: 0.0092 | LR: 4.01e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1300 | Loss: 0.0293 | LR: 3.96e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1400 | Loss: 0.0522 | LR: 3.92e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1500 | Loss: 0.0063 | LR: 3.87e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1600 | Loss: 0.0456 | LR: 3.83e-04 | Œ≥: 1.800\n",
      "Epoch 26 | Batch 1700 | Loss: 0.0235 | LR: 3.78e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 26 Summary:\n",
      "  Train Loss: 0.0243 | Val Loss: 0.0066\n",
      "  Accuracy: 0.9954 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9909 | Precision: 0.9820 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 27 | Batch    0 | Loss: 0.0456 | LR: 3.76e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  100 | Loss: 0.0075 | LR: 3.72e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  200 | Loss: 0.0357 | LR: 3.68e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  300 | Loss: 0.0254 | LR: 3.63e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  400 | Loss: 0.0168 | LR: 3.59e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  500 | Loss: 0.0335 | LR: 3.54e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  600 | Loss: 0.0058 | LR: 3.50e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  700 | Loss: 0.0060 | LR: 3.46e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  800 | Loss: 0.0067 | LR: 3.41e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch  900 | Loss: 0.0103 | LR: 3.37e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1000 | Loss: 0.0061 | LR: 3.33e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1100 | Loss: 0.0072 | LR: 3.28e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1200 | Loss: 0.0479 | LR: 3.24e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1300 | Loss: 0.0058 | LR: 3.20e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1400 | Loss: 0.0128 | LR: 3.16e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1500 | Loss: 0.0065 | LR: 3.12e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1600 | Loss: 0.0412 | LR: 3.08e-04 | Œ≥: 1.800\n",
      "Epoch 27 | Batch 1700 | Loss: 0.0152 | LR: 3.03e-04 | Œ≥: 1.800\n",
      "üìä SWA model updated (n_averaged: 3)\n",
      "\n",
      "Epoch 27 Summary:\n",
      "  Train Loss: 0.0236 | Val Loss: 0.0066\n",
      "  Accuracy: 0.9956 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9913 | Precision: 0.9830 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999926)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 28 | Batch    0 | Loss: 0.0261 | LR: 3.02e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  100 | Loss: 0.0140 | LR: 2.98e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  200 | Loss: 0.0064 | LR: 2.94e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  300 | Loss: 0.0470 | LR: 2.89e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  400 | Loss: 0.0064 | LR: 2.85e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  500 | Loss: 0.0523 | LR: 2.81e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  600 | Loss: 0.0198 | LR: 2.78e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  700 | Loss: 0.0181 | LR: 2.74e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  800 | Loss: 0.0067 | LR: 2.70e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch  900 | Loss: 0.0299 | LR: 2.66e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1000 | Loss: 0.0509 | LR: 2.62e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1100 | Loss: 0.0272 | LR: 2.58e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1200 | Loss: 0.0064 | LR: 2.54e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1300 | Loss: 0.0061 | LR: 2.50e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1400 | Loss: 0.0188 | LR: 2.47e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1500 | Loss: 0.0373 | LR: 2.43e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1600 | Loss: 0.0507 | LR: 2.39e-04 | Œ≥: 1.800\n",
      "Epoch 28 | Batch 1700 | Loss: 0.0487 | LR: 2.35e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 28 Summary:\n",
      "  Train Loss: 0.0240 | Val Loss: 0.0068\n",
      "  Accuracy: 0.9952 | ROC-AUC: 0.9999 | PR-AUC: 0.9997\n",
      "  F1: 0.9904 | Precision: 0.9812 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 29 | Batch    0 | Loss: 0.0429 | LR: 2.34e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  100 | Loss: 0.0060 | LR: 2.30e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  200 | Loss: 0.0363 | LR: 2.27e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  300 | Loss: 0.0256 | LR: 2.23e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  400 | Loss: 0.0058 | LR: 2.19e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  500 | Loss: 0.0106 | LR: 2.16e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  600 | Loss: 0.0504 | LR: 2.12e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  700 | Loss: 0.0089 | LR: 2.09e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  800 | Loss: 0.0109 | LR: 2.05e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch  900 | Loss: 0.0233 | LR: 2.02e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1000 | Loss: 0.0492 | LR: 1.98e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1100 | Loss: 0.0082 | LR: 1.95e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1200 | Loss: 0.0199 | LR: 1.92e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1300 | Loss: 0.0490 | LR: 1.88e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1400 | Loss: 0.0490 | LR: 1.85e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1500 | Loss: 0.0460 | LR: 1.82e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1600 | Loss: 0.0065 | LR: 1.78e-04 | Œ≥: 1.800\n",
      "Epoch 29 | Batch 1700 | Loss: 0.0399 | LR: 1.75e-04 | Œ≥: 1.800\n",
      "\n",
      "Epoch 29 Summary:\n",
      "  Train Loss: 0.0231 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9967 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9935 | Precision: 0.9873 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30 | Batch    0 | Loss: 0.0060 | LR: 1.74e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  100 | Loss: 0.0079 | LR: 1.70e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  200 | Loss: 0.0071 | LR: 1.67e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  300 | Loss: 0.0059 | LR: 1.64e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  400 | Loss: 0.0060 | LR: 1.61e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  500 | Loss: 0.0062 | LR: 1.58e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  600 | Loss: 0.0067 | LR: 1.55e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  700 | Loss: 0.0063 | LR: 1.52e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  800 | Loss: 0.0336 | LR: 1.49e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch  900 | Loss: 0.0368 | LR: 1.46e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1000 | Loss: 0.0064 | LR: 1.43e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1100 | Loss: 0.0079 | LR: 1.40e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1200 | Loss: 0.0175 | LR: 1.37e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1300 | Loss: 0.0443 | LR: 1.34e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1400 | Loss: 0.0359 | LR: 1.31e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1500 | Loss: 0.0061 | LR: 1.28e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1600 | Loss: 0.0062 | LR: 1.26e-04 | Œ≥: 1.800\n",
      "Epoch 30 | Batch 1700 | Loss: 0.0057 | LR: 1.23e-04 | Œ≥: 1.800\n",
      "üìä SWA model updated (n_averaged: 4)\n",
      "\n",
      "Epoch 30 Summary:\n",
      "  Train Loss: 0.0220 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9967 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9935 | Precision: 0.9873 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999931)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 31 | Batch    0 | Loss: 0.0134 | LR: 1.22e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  100 | Loss: 0.0293 | LR: 1.19e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  200 | Loss: 0.0063 | LR: 1.16e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  300 | Loss: 0.0187 | LR: 1.14e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  400 | Loss: 0.0435 | LR: 1.11e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  500 | Loss: 0.0069 | LR: 1.08e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  600 | Loss: 0.0470 | LR: 1.06e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  700 | Loss: 0.0059 | LR: 1.03e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  800 | Loss: 0.0240 | LR: 1.01e-04 | Œ≥: 1.800\n",
      "Epoch 31 | Batch  900 | Loss: 0.0305 | LR: 9.83e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1000 | Loss: 0.0070 | LR: 9.58e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1100 | Loss: 0.0064 | LR: 9.34e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1200 | Loss: 0.0122 | LR: 9.10e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1300 | Loss: 0.0065 | LR: 8.86e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1400 | Loss: 0.0124 | LR: 8.63e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1500 | Loss: 0.0264 | LR: 8.40e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1600 | Loss: 0.0503 | LR: 8.17e-05 | Œ≥: 1.800\n",
      "Epoch 31 | Batch 1700 | Loss: 0.0398 | LR: 7.94e-05 | Œ≥: 1.800\n",
      "\n",
      "Epoch 31 Summary:\n",
      "  Train Loss: 0.0239 | Val Loss: 0.0066\n",
      "  Accuracy: 0.9964 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9929 | Precision: 0.9860 | Recall: 0.9999\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 32 | Batch    0 | Loss: 0.0061 | LR: 7.85e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  100 | Loss: 0.0170 | LR: 7.63e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  200 | Loss: 0.0466 | LR: 7.41e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  300 | Loss: 0.0330 | LR: 7.20e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  400 | Loss: 0.0061 | LR: 6.98e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  500 | Loss: 0.0306 | LR: 6.77e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  600 | Loss: 0.0278 | LR: 6.57e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  700 | Loss: 0.0069 | LR: 6.37e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  800 | Loss: 0.0470 | LR: 6.17e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch  900 | Loss: 0.0359 | LR: 5.97e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1000 | Loss: 0.0329 | LR: 5.78e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1100 | Loss: 0.0065 | LR: 5.59e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1200 | Loss: 0.0525 | LR: 5.40e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1300 | Loss: 0.0065 | LR: 5.22e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1400 | Loss: 0.0062 | LR: 5.04e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1500 | Loss: 0.0337 | LR: 4.86e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1600 | Loss: 0.0503 | LR: 4.68e-05 | Œ≥: 1.800\n",
      "Epoch 32 | Batch 1700 | Loss: 0.0068 | LR: 4.51e-05 | Œ≥: 1.800\n",
      "\n",
      "Epoch 32 Summary:\n",
      "  Train Loss: 0.0229 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9969 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9939 | Precision: 0.9880 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999932)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 33 | Batch    0 | Loss: 0.0066 | LR: 4.44e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  100 | Loss: 0.0072 | LR: 4.27e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  200 | Loss: 0.0074 | LR: 4.11e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  300 | Loss: 0.0061 | LR: 3.95e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  400 | Loss: 0.0068 | LR: 3.79e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  500 | Loss: 0.0523 | LR: 3.64e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  600 | Loss: 0.0059 | LR: 3.48e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  700 | Loss: 0.0434 | LR: 3.34e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  800 | Loss: 0.0066 | LR: 3.19e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch  900 | Loss: 0.0534 | LR: 3.05e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1000 | Loss: 0.0479 | LR: 2.91e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1100 | Loss: 0.0156 | LR: 2.78e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1200 | Loss: 0.0058 | LR: 2.64e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1300 | Loss: 0.0172 | LR: 2.51e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1400 | Loss: 0.0394 | LR: 2.39e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1500 | Loss: 0.0179 | LR: 2.27e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1600 | Loss: 0.0059 | LR: 2.15e-05 | Œ≥: 1.800\n",
      "Epoch 33 | Batch 1700 | Loss: 0.0350 | LR: 2.03e-05 | Œ≥: 1.800\n",
      "üìä SWA model updated (n_averaged: 5)\n",
      "\n",
      "Epoch 33 Summary:\n",
      "  Train Loss: 0.0236 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9967 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9935 | Precision: 0.9872 | Recall: 0.9998\n",
      "  üíæ New best model saved! (AUC: 0.999934)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 34 | Batch    0 | Loss: 0.0071 | LR: 1.98e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  100 | Loss: 0.0418 | LR: 1.87e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  200 | Loss: 0.0090 | LR: 1.76e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  300 | Loss: 0.0139 | LR: 1.66e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  400 | Loss: 0.0057 | LR: 1.55e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  500 | Loss: 0.0071 | LR: 1.45e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  600 | Loss: 0.0063 | LR: 1.36e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  700 | Loss: 0.0507 | LR: 1.27e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  800 | Loss: 0.0295 | LR: 1.18e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch  900 | Loss: 0.0110 | LR: 1.09e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1000 | Loss: 0.0080 | LR: 1.01e-05 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1100 | Loss: 0.0060 | LR: 9.29e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1200 | Loss: 0.0366 | LR: 8.53e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1300 | Loss: 0.0065 | LR: 7.80e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1400 | Loss: 0.0101 | LR: 7.10e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1500 | Loss: 0.0352 | LR: 6.44e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1600 | Loss: 0.0295 | LR: 5.81e-06 | Œ≥: 1.800\n",
      "Epoch 34 | Batch 1700 | Loss: 0.0062 | LR: 5.21e-06 | Œ≥: 1.800\n",
      "\n",
      "Epoch 34 Summary:\n",
      "  Train Loss: 0.0233 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9968 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9936 | Precision: 0.9874 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35 | Batch    0 | Loss: 0.0093 | LR: 4.97e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  100 | Loss: 0.0063 | LR: 4.41e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  200 | Loss: 0.0265 | LR: 3.89e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  300 | Loss: 0.0301 | LR: 3.40e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  400 | Loss: 0.0065 | LR: 2.95e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  500 | Loss: 0.0237 | LR: 2.53e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  600 | Loss: 0.0112 | LR: 2.14e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  700 | Loss: 0.0078 | LR: 1.78e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  800 | Loss: 0.0119 | LR: 1.46e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch  900 | Loss: 0.0394 | LR: 1.16e-06 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1000 | Loss: 0.0072 | LR: 9.05e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1100 | Loss: 0.0517 | LR: 6.79e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1200 | Loss: 0.0093 | LR: 4.86e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1300 | Loss: 0.0066 | LR: 3.25e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1400 | Loss: 0.0064 | LR: 1.97e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1500 | Loss: 0.0127 | LR: 1.02e-07 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1600 | Loss: 0.0508 | LR: 4.01e-08 | Œ≥: 1.800\n",
      "Epoch 35 | Batch 1700 | Loss: 0.0104 | LR: 1.06e-08 | Œ≥: 1.800\n",
      "\n",
      "Epoch 35 Summary:\n",
      "  Train Loss: 0.0238 | Val Loss: 0.0065\n",
      "  Accuracy: 0.9968 | ROC-AUC: 0.9999 | PR-AUC: 0.9998\n",
      "  F1: 0.9936 | Precision: 0.9874 | Recall: 0.9998\n",
      "--------------------------------------------------------------------------------\n",
      "üîÑ Evaluating SWA model...\n",
      "üìä SWA Model AUC: 0.999930\n",
      "\n",
      "üìä FINAL ENHANCED EVALUATION (NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING, NO EARLY STOPPING):\n",
      "====================================================================================================\n",
      "üèÜ Best Validation ROC-AUC: 0.999934\n",
      "üéØ Target achieved: ‚úÖ (Target: 99.80%+)\n",
      "\n",
      "üìã Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9999    0.9957    0.9978    334588\n",
      "      Attack     0.9874    0.9998    0.9936    111529\n",
      "\n",
      "    accuracy                         0.9968    446117\n",
      "   macro avg     0.9936    0.9978    0.9957    446117\n",
      "weighted avg     0.9968    0.9968    0.9968    446117\n",
      "\n",
      "\n",
      "üìä Final Confusion Matrix:\n",
      "             Pred Normal  Pred Attack\n",
      "True Normal       333160         1428\n",
      "True Attack           19       111510\n",
      "\n",
      "üîí Security Metrics:\n",
      "  False Positive Rate: 0.004268 (1,428 false alarms)\n",
      "  False Negative Rate: 0.000170 (19 missed attacks)\n",
      "  Attack Detection Rate: 0.999830\n",
      "\n",
      "üìà Performance Improvements (NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING, NO EARLY STOPPING):\n",
      "  ‚úÖ Enhanced Architecture: Feature attention + residual scaling\n",
      "  ‚úÖ Optimal Hyperparameters: d_model=160, Œ≥=1.8, Œ±=0.75\n",
      "  ‚úÖ Advanced Training: SWA + Mixup + Adaptive Loss\n",
      "  ‚úÖ Intelligent Undersampling: Distance-based sampling only\n",
      "  ‚úÖ Multi-GPU Optimization: T4 x2 support\n",
      "  ‚úÖ Full Training: 35 epochs without early stopping\n",
      "  ‚úÖ Original Features Only: No feature engineering, no synthetic samples\n",
      "\n",
      "üíæ Enhanced model saved to: /kaggle/working/enhanced_binary_rtids_model_no_synthetic_no_features_no_early_stopping.pth\n",
      "üöÄ Enhanced training completed successfully!\n",
      "üéØ Full 35-epoch training with original features only (no synthetic samples, no feature engineering, no early stopping)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced Binary RTIDS Training Script - GPU T4 x2 Optimized (Kaggle Compatible)\n",
    "- Implements ALL performance improvements without imblearn dependency\n",
    "- Uses optimal hyperparameters: d_model=160, focal_gamma=1.8, focal_alpha=0.75\n",
    "- Advanced architecture with residual scaling and attention pooling\n",
    "- Custom balancing (undersampling only, NO synthetic samples, NO feature engineering, NO early stopping)\n",
    "- Expected performance: 99.85%+ ROC-AUC\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from torch.nn.parallel import DataParallel\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, precision_recall_curve, auc,\n",
    "                           f1_score, precision_score, recall_score)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Multi-GPU Setup for T4 x2\n",
    "def setup_device():\n",
    "    \"\"\"Setup multi-GPU device configuration for T4 x2\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"üî• CUDA available with {device_count} GPU(s)\")\n",
    "        \n",
    "        for i in range(device_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"   GPU {i}: {gpu_name} ({memory:.1f}GB)\")\n",
    "        \n",
    "        device = torch.device(\"cuda:0\")\n",
    "        return device, device_count > 1\n",
    "    else:\n",
    "        print(\"üíª CUDA not available, using CPU\")\n",
    "        return torch.device(\"cpu\"), False\n",
    "\n",
    "DEVICE, MULTI_GPU = setup_device()\n",
    "\n",
    "# ======================================================================================\n",
    "# ENHANCED CONFIGURATION WITH OPTIMAL HYPERPARAMETERS\n",
    "# ======================================================================================\n",
    "class EnhancedConfig:\n",
    "    def __init__(self):\n",
    "        # Kaggle paths\n",
    "        self.input_path = '/kaggle/input/cicids2017/cicids2017.csv'\n",
    "        self.output_dir = '/kaggle/working/'\n",
    "        \n",
    "        # OPTIMAL TRAINING PARAMETERS (from experimental results)\n",
    "        self.epochs = 35  # Extended for better convergence\n",
    "        self.batch_size = 1024 if MULTI_GPU else 512  # Optimal from experiments\n",
    "        self.val_batch_size = 2048 if MULTI_GPU else 1024\n",
    "        \n",
    "        # OPTIMAL MODEL ARCHITECTURE (best from Phase 2)\n",
    "        self.d_model = 160  # Best performing architecture\n",
    "        self.num_layers = 4\n",
    "        self.heads = 10\n",
    "        self.d_ff = 640\n",
    "        self.dropout = 0.15\n",
    "        \n",
    "        # OPTIMAL OPTIMIZATION (from Phase 4)\n",
    "        self.lr = 0.002  # Best learning rate\n",
    "        self.weight_decay = 1e-4\n",
    "        self.warmup_steps = 1000\n",
    "        \n",
    "        # OPTIMAL LOSS CONFIGURATION (from Phase 3)\n",
    "        self.focal_gamma = 1.8  # Best focal gamma\n",
    "        self.focal_alpha = 0.75  # Best focal alpha\n",
    "        self.use_class_weights = True\n",
    "        self.label_smoothing = 0.1\n",
    "        \n",
    "        # OPTIMAL DATA HANDLING (from Phase 1) - NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING\n",
    "        self.test_size = 0.2\n",
    "        self.random_state = 42\n",
    "        self.use_robust_scaling = True\n",
    "        self.undersampling_ratio = 0.12  # Slightly higher ratio since no SMOTE\n",
    "        \n",
    "        # ADVANCED TRAINING FEATURES\n",
    "        self.use_swa = True\n",
    "        self.swa_start = 20\n",
    "        self.swa_freq = 3\n",
    "        self.use_mixup = True\n",
    "        self.mixup_alpha = 0.2\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        \n",
    "        # Multi-GPU specific\n",
    "        self.use_multi_gpu = MULTI_GPU\n",
    "        self.num_workers = 6 if MULTI_GPU else 4\n",
    "\n",
    "# ======================================================================================\n",
    "# CUSTOM DATA PROCESSING (NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING)\n",
    "# ======================================================================================\n",
    "class IntelligentDataBalancer:\n",
    "    \"\"\"Intelligent data balancing using undersampling only (NO synthetic samples)\"\"\"\n",
    "    def __init__(self, undersampling_ratio=0.12, random_state=42):\n",
    "        self.undersampling_ratio = undersampling_ratio\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def balance_classes(self, X, y):\n",
    "        \"\"\" Advanced balancing using intelligent undersampling only\"\"\"\n",
    "        print(\"‚öñÔ∏è Intelligent class balancing (undersampling only)...\")\n",
    "        \n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"Original distribution: {dict(zip(['Normal', 'Attack'], counts))}\")\n",
    "        \n",
    "        # Intelligent undersampling\n",
    "        majority_indices = np.where(y == 0)[0]\n",
    "        minority_indices = np.where(y == 1)[0]\n",
    "        \n",
    "        # Calculate target majority size for better balance\n",
    "        target_majority = max(\n",
    "            len(minority_indices) * 3,  # 3:1 ratio for better balance\n",
    "            int(len(majority_indices) * self.undersampling_ratio)\n",
    "        )\n",
    "        \n",
    "        if len(majority_indices) > target_majority:\n",
    "            # Sample majority class with slight preference for samples closer to minority class\n",
    "            try:\n",
    "                # Quick distance-based sampling for better boundary representation\n",
    "                minority_samples = X[minority_indices]\n",
    "                majority_samples = X[majority_indices]\n",
    "                \n",
    "                # Calculate distances to minority class center\n",
    "                minority_center = np.mean(minority_samples, axis=0)\n",
    "                distances = np.linalg.norm(majority_samples - minority_center, axis=1)\n",
    "                \n",
    "                # Create sampling weights (closer samples have higher probability)\n",
    "                weights = 1 / (distances + 1e-8)\n",
    "                weights = weights / np.sum(weights)\n",
    "                \n",
    "                selected_majority = np.random.choice(\n",
    "                    majority_indices, \n",
    "                    size=target_majority, \n",
    "                    replace=False,\n",
    "                    p=weights\n",
    "                )\n",
    "                print(f\"‚úÖ Applied distance-based intelligent undersampling\")\n",
    "            except:\n",
    "                # Fallback to random sampling\n",
    "                selected_majority = np.random.choice(\n",
    "                    majority_indices, size=target_majority, replace=False\n",
    "                )\n",
    "                print(f\"‚úÖ Applied random undersampling\")\n",
    "        else:\n",
    "            selected_majority = majority_indices\n",
    "        \n",
    "        # Combine undersampled data\n",
    "        selected_indices = np.concatenate([selected_majority, minority_indices])\n",
    "        X_balanced = X[selected_indices]\n",
    "        y_balanced = y[selected_indices]\n",
    "        \n",
    "        final_counts = np.bincount(y_balanced)\n",
    "        print(f\"Final distribution: Normal={final_counts[0]:,}, Attack={final_counts[1]:,}\")\n",
    "        print(f\"Class ratio: {final_counts[0]/final_counts[1]:.2f}:1 (Normal:Attack)\")\n",
    "        \n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "class RobustPreprocessor:\n",
    "    def __init__(self, scaling_method='quantile', handle_outliers=True, n_features=120):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.n_features = n_features\n",
    "        self.scaler = None\n",
    "        self.feature_selector = None\n",
    "        self.outlier_detector = None\n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Robust preprocessing pipeline\"\"\"\n",
    "        print(\"üîß Advanced preprocessing...\")\n",
    "        \n",
    "        # Handle infinite and missing values\n",
    "        X_processed = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Intelligent missing value imputation\n",
    "        for col in X_processed.columns:\n",
    "            if X_processed[col].isna().sum() > 0:\n",
    "                if y is not None:\n",
    "                    # Class-specific imputation\n",
    "                    for class_val in np.unique(y):\n",
    "                        mask = (y == class_val) & X_processed[col].notna()\n",
    "                        if mask.sum() > 0:\n",
    "                            fill_value = X_processed.loc[mask, col].median()\n",
    "                            class_mask = (y == class_val) & X_processed[col].isna()\n",
    "                            X_processed.loc[class_mask, col] = fill_value\n",
    "                else:\n",
    "                    X_processed[col].fillna(X_processed[col].median(), inplace=True)\n",
    "        \n",
    "        # Outlier handling\n",
    "        if self.handle_outliers:\n",
    "            try:\n",
    "                # Simple quantile-based outlier capping\n",
    "                for col in X_processed.columns:\n",
    "                    Q1 = X_processed[col].quantile(0.005)\n",
    "                    Q99 = X_processed[col].quantile(0.995)\n",
    "                    X_processed[col] = X_processed[col].clip(lower=Q1, upper=Q99)\n",
    "                print(\"‚úÖ Outlier capping applied\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Outlier handling failed: {e}\")\n",
    "        \n",
    "        # Feature selection\n",
    "        if y is not None and self.n_features < X_processed.shape[1]:\n",
    "            try:\n",
    "                self.feature_selector = SelectKBest(mutual_info_classif, k=self.n_features)\n",
    "                X_selected = self.feature_selector.fit_transform(X_processed, y)\n",
    "                X_processed = pd.DataFrame(X_selected, index=X_processed.index)\n",
    "                print(f\"üìä Selected {self.n_features} most informative features\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Feature selection failed: {e}\")\n",
    "        \n",
    "        # Advanced scaling\n",
    "        try:\n",
    "            if self.scaling_method == 'quantile':\n",
    "                self.scaler = QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "            elif self.scaling_method == 'robust':\n",
    "                self.scaler = RobustScaler()\n",
    "            else:\n",
    "                self.scaler = StandardScaler()\n",
    "            \n",
    "            X_scaled = self.scaler.fit_transform(X_processed)\n",
    "            print(f\"‚úÖ Applied {self.scaling_method} scaling\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Scaling failed: {e}\")\n",
    "            X_scaled = X_processed.values\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform new data using fitted preprocessor\"\"\"\n",
    "        X_processed = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        for col in X_processed.columns:\n",
    "            if X_processed[col].isna().sum() > 0:\n",
    "                X_processed[col].fillna(X_processed[col].median(), inplace=True)\n",
    "        \n",
    "        if self.handle_outliers:\n",
    "            for col in X_processed.columns:\n",
    "                Q1 = X_processed[col].quantile(0.005)\n",
    "                Q99 = X_processed[col].quantile(0.995)\n",
    "                X_processed[col] = X_processed[col].clip(lower=Q1, upper=Q99)\n",
    "        \n",
    "        if self.feature_selector is not None:\n",
    "            X_processed = self.feature_selector.transform(X_processed)\n",
    "            X_processed = pd.DataFrame(X_processed)\n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            return self.scaler.transform(X_processed)\n",
    "        else:\n",
    "            return X_processed.values\n",
    "\n",
    "# ======================================================================================\n",
    "# TRANSFORMER ARCHITECTURE\n",
    "# ======================================================================================\n",
    "class FeatureImportanceLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.projection = nn.Linear(input_dim, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Calculate feature importance scores\n",
    "        # Input: [batch_size, 78]\n",
    "        # Through: Linear(78‚Üí160) ‚Üí Tanh ‚Üí Linear(160‚Üí78) ‚Üí Sigmoid\n",
    "        # Output: [batch_size, 78] (importance scores between 0-1)\n",
    "        importance_scores = self.feature_attention(x)\n",
    "        \n",
    "        # Apply attention to input features\n",
    "        attended_features = x * importance_scores\n",
    "        \n",
    "        # Project to model dimension\n",
    "        embedded = self.projection(attended_features)\n",
    "        normalized = self.layer_norm(embedded)\n",
    "        embedded = self.dropout(normalized)\n",
    "\n",
    "        # Add sequence dimension for transformer processing\n",
    "        return embedded.unsqueeze(1), importance_scores\n",
    "        # Transformers expect sequences, but we have tabular data\n",
    "        # We treat each sample as a \"sequence\" of length 1\n",
    "        # This allows transformer attention mechanisms to work\n",
    "\n",
    "class EnhancedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(d_model, d_model * 3, bias=False)\n",
    "        self.output_projection = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv_projection(x) # Linear(160 ‚Üí 480)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.d_k) # 160/10 = 16 per head\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        #q = q.reshape(batch_size, 1, 10, 16)  # 160/10 = 16 per head\n",
    "        # k = k.reshape(batch_size, 1, 10, 16)\n",
    "        # v = v.reshape(batch_size, 1, 10, 16)\n",
    "        \n",
    "        # Attention computation Each attention head focuses on different feature relationships\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.reshape(batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.output_projection(attention_output)\n",
    "\n",
    "class EnhancedTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = EnhancedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), # Expand dimension\n",
    "            nn.GELU(),                # Activation function\n",
    "            nn.Dropout(dropout),      # Regularization\n",
    "            nn.Linear(d_ff, d_model)  # Project back to original dimension\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Residual scaling for better gradient flow\n",
    "        self.residual_scale = nn.Parameter(torch.ones(1) * 0.8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Enhanced residual connections with scaling\n",
    "        #Layer Normalization \n",
    "        attended = self.attention(self.norm1(x))\n",
    "        x = x + self.dropout(attended) * self.residual_scale\n",
    "        \n",
    "        fed_forward = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(fed_forward) * self.residual_scale\n",
    "        \n",
    "        return x\n",
    "                                    # Block 1: Learns basic feature patterns\n",
    "                                    # Block 2: Learns feature interactions\n",
    "                                    # Block 3: Learns complex attack signatures\n",
    "                                    # Block 4: Refines final representations\n",
    "class AttentionPoolingClassifier(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.15):\n",
    "        super().__init__()\n",
    "        # Attention-based pooling\n",
    "        self.attention_pool = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Enhanced classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2), # First reduction\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model // 4), # Second reduction\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 4, 2)      # Final output for binary classification [Normal, Attack]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Add learnable CLS token\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x_with_cls = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # Attention pooling\n",
    "        pooled, _ = self.attention_pool(cls_token, x_with_cls, x_with_cls)\n",
    "        pooled = pooled.squeeze(1)\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class EnhancedBinaryTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=160, num_layers=4, num_heads=10, d_ff=640, dropout=0.15):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Enhanced feature embedding with importance\n",
    "        self.feature_embedder = FeatureImportanceLayer(input_dim, d_model)\n",
    "        \n",
    "        # Enhanced transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            EnhancedTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Enhanced classifier with attention pooling\n",
    "        self.classifier = AttentionPoolingClassifier(d_model, dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.constant_(module.bias, 0)\n",
    "            torch.nn.init.constant_(module.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Enhanced feature embedding with importance\n",
    "        x, feature_importance = self.feature_embedder(x)\n",
    "        \n",
    "        # Pass through enhanced transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Enhanced classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits, feature_importance\n",
    "\n",
    "# ======================================================================================\n",
    "# ADVANCED TRAINING COMPONENTS\n",
    "# ======================================================================================\n",
    "class AdaptiveFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma_min=1.0, gamma_max=3.0, \n",
    "                 class_weights=None, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma_min = gamma_min\n",
    "        self.gamma_max = gamma_max\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Learnable gamma parameter (initialized to optimal value)\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.8))\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Clamp gamma to reasonable range\n",
    "        gamma = torch.clamp(self.gamma, self.gamma_min, self.gamma_max)\n",
    "        \n",
    "        # Standard focal loss computation\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.class_weights,\n",
    "                                 reduction='none', label_smoothing=self.label_smoothing)\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            alpha_weight = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "            focal_loss = alpha_weight * (1 - pt) ** gamma * ce_loss\n",
    "        else:\n",
    "            focal_loss = (1 - pt) ** gamma * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "                #Why Focal Loss?\n",
    "                # Handles class imbalance (more normal than attack traffic)\n",
    "                # Focuses on hard-to-classify examples\n",
    "                # Œ±=0.75 gives more weight to attack class\n",
    "                # Œ≥=1.8 reduces loss for easy examples\n",
    "\n",
    "class TabularMixup:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "    def mixup_criterion(self, pred, y_a, y_b, lam, criterion):\n",
    "        if isinstance(pred, tuple):\n",
    "            pred, _ = pred\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class SWAOptimizer:\n",
    "    def __init__(self, base_optimizer, swa_start=20, swa_freq=3, swa_lr=0.001):\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.swa_start = swa_start\n",
    "        self.swa_freq = swa_freq\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_model = None\n",
    "        self.n_averaged = 0\n",
    "        \n",
    "    def update_swa(self, model, epoch):\n",
    "        if epoch >= self.swa_start and (epoch - self.swa_start) % self.swa_freq == 0:\n",
    "            if self.swa_model is None:\n",
    "                self.swa_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                # Update SWA model\n",
    "                for swa_param, param in zip(self.swa_model.parameters(), model.parameters()):\n",
    "                    swa_param.data = (swa_param.data * self.n_averaged + param.data) / (self.n_averaged + 1)\n",
    "            self.n_averaged += 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_swa_model(self):\n",
    "        return self.swa_model\n",
    "\n",
    "# ======================================================================================\n",
    "# ENHANCED TRAINING UTILITIES\n",
    "# ======================================================================================\n",
    "def get_device_and_model(model, config):\n",
    "    \"\"\"Setup device and model with multi-GPU support\"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    if config.use_multi_gpu and MULTI_GPU:\n",
    "        print(f\"üî• Using DataParallel with {torch.cuda.device_count()} GPUs\")\n",
    "        model = DataParallel(model)\n",
    "    \n",
    "    print(f\"üíª Using device: {DEVICE}\")\n",
    "    if config.use_multi_gpu and MULTI_GPU:\n",
    "        print(f\"üî• Multi-GPU mode enabled\")\n",
    "    \n",
    "    return DEVICE, model\n",
    "\n",
    "def create_enhanced_data_loaders(X_train, y_train, X_val, y_val, config):\n",
    "    \"\"\"Create optimized data loaders\"\"\"\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.LongTensor(y_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.LongTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "    \n",
    "    # Enhanced weighted sampler\n",
    "    class_counts = np.bincount(y_train)\n",
    "    weights = 1.0 / class_counts\n",
    "    sample_weights = weights[y_train]\n",
    "    \n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        torch.DoubleTensor(sample_weights),\n",
    "        len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Optimized data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=weighted_sampler,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True if config.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if config.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_prob) == 0:\n",
    "        return {\n",
    "            'accuracy': 0.0, 'auc_roc': 0.0, 'auc_pr': 0.0,\n",
    "            'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        accuracy = np.mean(y_true == y_pred)\n",
    "        \n",
    "        unique_classes = np.unique(y_true)\n",
    "        if len(unique_classes) < 2:\n",
    "            return {\n",
    "                'accuracy': accuracy, 'auc_roc': 0.5, 'auc_pr': np.mean(y_true),\n",
    "                'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0\n",
    "            }\n",
    "        \n",
    "        auc_roc = roc_auc_score(y_true, y_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "        auc_pr = auc(recall, precision)\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc_roc': auc_roc,\n",
    "            'auc_pr': auc_pr,\n",
    "            'f1_score': f1,\n",
    "            'precision': prec,\n",
    "            'recall': rec\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error calculating metrics: {e}\")\n",
    "        return {\n",
    "            'accuracy': 0.0, 'auc_roc': 0.0, 'auc_pr': 0.0,\n",
    "            'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0\n",
    "        }\n",
    "\n",
    "# ======================================================================================\n",
    "# MAIN ENHANCED TRAINING FUNCTION\n",
    "# ======================================================================================\n",
    "def main():\n",
    "    config = EnhancedConfig()\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(config.random_state)\n",
    "    np.random.seed(config.random_state)\n",
    "    random.seed(config.random_state)\n",
    "    \n",
    "    print(\"üöÄ Enhanced Binary RTIDS Training - GPU T4 x2 Optimized (No Synthetic Samples, No Feature Engineering, No Early Stopping)\")\n",
    "    print(\"üéØ Target: 99.85%+ ROC-AUC with optimal hyperparameters\")\n",
    "    print(\"‚úÖ Full epoch training - NO synthetic samples, NO feature engineering, NO early stopping\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"üìä Loading CICIDS2017 dataset...\")\n",
    "    df = pd.read_csv(config.input_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Find label column\n",
    "    label_col = None\n",
    "    for col in df.columns:\n",
    "        if 'label' in col.lower():\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    if label_col is None:\n",
    "        raise ValueError(\"‚ùå No label column found!\")\n",
    "    \n",
    "    print(f\"‚úÖ Found label column: {label_col}\")\n",
    "    \n",
    "    # Convert to binary classification\n",
    "    print(\"üîÑ Converting to binary classification...\")\n",
    "    df['binary_label'] = (df[label_col] != 'BENIGN').astype(int)\n",
    "    \n",
    "    print(f\"Binary distribution:\")\n",
    "    print(f\"Normal (0): {np.sum(df['binary_label'] == 0):,}\")\n",
    "    print(f\"Attack (1): {np.sum(df['binary_label'] == 1):,}\")\n",
    "    \n",
    "    # Prepare features (using original features only)\n",
    "    print(\"üìä Using original features only...\")\n",
    "    exclude_cols = [label_col, 'binary_label', 'Flow ID', 'Source IP', \n",
    "                   'Destination IP', 'Timestamp']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['binary_label'].values\n",
    "    \n",
    "    print(f\"Using {len(feature_cols)} original features (no feature engineering)\")\n",
    "    \n",
    "    # Handle missing and infinite values\n",
    "    print(\"üßπ Cleaning data...\")\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Advanced preprocessing\n",
    "    print(\"üìè Advanced preprocessing...\")\n",
    "    preprocessor = RobustPreprocessor(\n",
    "        scaling_method='quantile',\n",
    "        handle_outliers=True,\n",
    "        n_features=120\n",
    "    )\n",
    "    X_processed = preprocessor.fit_transform(X, y)\n",
    "    \n",
    "    # Intelligent class balancing (undersampling only)\n",
    "    print(\"‚öñÔ∏è Intelligent class balancing (undersampling only)...\")\n",
    "    balancer = IntelligentDataBalancer(undersampling_ratio=config.undersampling_ratio, random_state=config.random_state)\n",
    "    X_balanced, y_balanced = balancer.balance_classes(X_processed, y)\n",
    "    \n",
    "    # Train-validation split\n",
    "    print(\"‚úÇÔ∏è Splitting data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_balanced, y_balanced,\n",
    "        test_size=config.test_size,\n",
    "        stratify=y_balanced,\n",
    "        random_state=config.random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]:,} samples\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    class_weights = torch.FloatTensor([\n",
    "        total_samples / (2 * class_counts[0]),\n",
    "        total_samples / (2 * class_counts[1])\n",
    "    ])\n",
    "    \n",
    "    print(f\"üìä Class weights: Normal={class_weights[0]:.3f}, Attack={class_weights[1]:.3f}\")\n",
    "    \n",
    "    # Create enhanced model\n",
    "    print(\"ü§ñ Creating Enhanced Binary Transformer...\")\n",
    "    model = EnhancedBinaryTransformerClassifier(\n",
    "        input_dim=X_train.shape[1],\n",
    "        d_model=config.d_model,\n",
    "        num_layers=config.num_layers,\n",
    "        num_heads=config.heads,\n",
    "        d_ff=config.d_ff,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    \n",
    "    # Setup device and multi-GPU\n",
    "    device, model = get_device_and_model(model, config)\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"üìà Total parameters: {total_params:,}\")\n",
    "    print(f\"üìà Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create enhanced data loaders\n",
    "    print(\"üîÑ Creating enhanced data loaders...\")\n",
    "    train_loader, val_loader = create_enhanced_data_loaders(\n",
    "        X_train, y_train, X_val, y_val, config\n",
    "    )\n",
    "    \n",
    "    # Advanced loss function and optimizer\n",
    "    criterion = AdaptiveFocalLoss(\n",
    "        alpha=config.focal_alpha,\n",
    "        gamma_min=1.0,\n",
    "        gamma_max=3.0,\n",
    "        class_weights=class_weights if config.use_class_weights else None,\n",
    "        label_smoothing=config.label_smoothing\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Enhanced learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config.lr,\n",
    "        epochs=config.epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # Advanced training components\n",
    "    mixup = TabularMixup(alpha=config.mixup_alpha) if config.use_mixup else None\n",
    "    swa_optimizer = SWAOptimizer(optimizer, swa_start=config.swa_start, swa_freq=config.swa_freq) if config.use_swa else None\n",
    "    \n",
    "    print(f\"üéØ Enhanced Loss: Adaptive Focal Loss (Œ≥=learnable, Œ±={config.focal_alpha})\")\n",
    "    print(f\"üéØ Optimizer: AdamW (lr={config.lr})\")\n",
    "    print(f\"üéØ Advanced Features: SWA={config.use_swa}, Mixup={config.use_mixup}\")\n",
    "    print(f\"üéØ Training: Full {config.epochs} epochs (no early stopping)\")\n",
    "    print(f\"üéØ Data Strategy: Original features + intelligent undersampling only (NO synthetic samples, NO feature engineering)\")\n",
    "    \n",
    "    # Enhanced training loop\n",
    "    print(\"\\nüöÄ Starting enhanced training...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_auc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply mixup\n",
    "            if mixup and epoch > 5:\n",
    "                mixed_data, y_a, y_b, lam = mixup(data, target)\n",
    "                output = model(mixed_data)\n",
    "                loss = mixup.mixup_criterion(output, y_a, y_b, lam, criterion)\n",
    "            else:\n",
    "                output = model(data)\n",
    "                if isinstance(output, tuple):\n",
    "                    output, _ = output\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                gamma_val = criterion.gamma.item() if hasattr(criterion, 'gamma') else config.focal_gamma\n",
    "                print(f\"Epoch {epoch+1:2d} | Batch {batch_idx:4d} | Loss: {loss.item():.4f} | LR: {current_lr:.2e} | Œ≥: {gamma_val:.3f}\")\n",
    "        \n",
    "        # SWA update\n",
    "        if swa_optimizer:\n",
    "            swa_updated = swa_optimizer.update_swa(model, epoch)\n",
    "            if swa_updated:\n",
    "                print(f\"üìä SWA model updated (n_averaged: {swa_optimizer.n_averaged})\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                output = model(data)\n",
    "                if isinstance(output, tuple):\n",
    "                    output, _ = output\n",
    "                \n",
    "                val_loss = criterion(output, target)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                probs = F.softmax(output, dim=1)\n",
    "                preds = output.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = calculate_comprehensive_metrics(\n",
    "            np.array(all_targets), \n",
    "            np.array(all_preds), \n",
    "            np.array(all_probs)\n",
    "        )\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1:2d} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f} | ROC-AUC: {metrics['auc_roc']:.4f} | PR-AUC: {metrics['auc_pr']:.4f}\")\n",
    "        print(f\"  F1: {metrics['f1_score']:.4f} | Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['auc_roc'] > best_auc:\n",
    "            best_auc = metrics['auc_roc']\n",
    "            model_to_save = model.module if isinstance(model, DataParallel) else model\n",
    "            best_model_state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'config': config.__dict__,\n",
    "                'preprocessor': preprocessor,\n",
    "                'balancer': balancer,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            model_path = os.path.join(config.output_dir, 'enhanced_binary_rtids_model_no_synthetic_no_features_no_early_stopping.pth')\n",
    "            torch.save(best_model_state, model_path)\n",
    "            print(f\"  üíæ New best model saved! (AUC: {best_auc:.6f})\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Use SWA model if available\n",
    "    if swa_optimizer and swa_optimizer.get_swa_model() is not None:\n",
    "        print(\"üîÑ Evaluating SWA model...\")\n",
    "        swa_model = swa_optimizer.get_swa_model()\n",
    "        \n",
    "        # Quick SWA evaluation\n",
    "        swa_model.eval()\n",
    "        swa_probs = []\n",
    "        swa_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = swa_model(data)\n",
    "                if isinstance(output, tuple):\n",
    "                    output, _ = output\n",
    "                probs = F.softmax(output, dim=1)[:, 1]\n",
    "                swa_probs.extend(probs.cpu().numpy())\n",
    "                swa_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        swa_auc = roc_auc_score(swa_targets, swa_probs)\n",
    "        print(f\"üìä SWA Model AUC: {swa_auc:.6f}\")\n",
    "        \n",
    "        if swa_auc > best_auc:\n",
    "            print(\"üèÜ SWA model is better! Using SWA for final model.\")\n",
    "            best_auc = swa_auc\n",
    "            best_model_state['model_state_dict'] = swa_model.state_dict()\n",
    "            best_model_state['best_auc'] = swa_auc\n",
    "            torch.save(best_model_state, model_path)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nüìä FINAL ENHANCED EVALUATION (NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING, NO EARLY STOPPING):\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"üèÜ Best Validation ROC-AUC: {best_auc:.6f}\")\n",
    "    print(f\"üéØ Target achieved: {'‚úÖ' if best_auc >= 0.998 else 'üîÑ'} (Target: 99.80%+)\")\n",
    "    \n",
    "    if len(all_targets) > 0 and len(np.unique(all_targets)) > 1:\n",
    "        print(\"\\nüìã Final Classification Report:\")\n",
    "        print(classification_report(\n",
    "            all_targets, all_preds,\n",
    "            target_names=['Normal', 'Attack'],\n",
    "            digits=4\n",
    "        ))\n",
    "        \n",
    "        print(\"\\nüìä Final Confusion Matrix:\")\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm, \n",
    "            index=['True Normal', 'True Attack'], \n",
    "            columns=['Pred Normal', 'Pred Attack']\n",
    "        )\n",
    "        print(cm_df)\n",
    "        \n",
    "        # Security-specific metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüîí Security Metrics:\")\n",
    "        print(f\"  False Positive Rate: {fpr:.6f} ({fp:,} false alarms)\")\n",
    "        print(f\"  False Negative Rate: {fnr:.6f} ({fn:,} missed attacks)\")\n",
    "        print(f\"  Attack Detection Rate: {1-fnr:.6f}\")\n",
    "        \n",
    "        # Performance improvement summary\n",
    "        print(f\"\\nüìà Performance Improvements (NO SYNTHETIC SAMPLES, NO FEATURE ENGINEERING, NO EARLY STOPPING):\")\n",
    "        print(f\"  ‚úÖ Enhanced Architecture: Feature attention + residual scaling\")\n",
    "        print(f\"  ‚úÖ Optimal Hyperparameters: d_model=160, Œ≥=1.8, Œ±=0.75\")\n",
    "        print(f\"  ‚úÖ Advanced Training: SWA + Mixup + Adaptive Loss\")\n",
    "        print(f\"  ‚úÖ Intelligent Undersampling: Distance-based sampling only\")\n",
    "        print(f\"  ‚úÖ Multi-GPU Optimization: T4 x2 support\")\n",
    "        print(f\"  ‚úÖ Full Training: {config.epochs} epochs without early stopping\")\n",
    "        print(f\"  ‚úÖ Original Features Only: No feature engineering, no synthetic samples\")\n",
    "    \n",
    "    print(f\"\\nüíæ Enhanced model saved to: {model_path}\")\n",
    "    print(\"üöÄ Enhanced training completed successfully!\")\n",
    "    print(f\"üéØ Full {config.epochs}-epoch training with original features only (no synthetic samples, no feature engineering, no early stopping)\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-Transformer Hybrid IDS Training + Interpretability\n",
    "# ===============================================================\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "import json\n",
    "import joblib\n",
    "RUN_CNN_TRANSFORMER = False  # Flip to True before running this cell to launch training\n",
    "\n",
    "@dataclass\n",
    "class CNNTransformerConfig:\n",
    "    input_path: str = \"/kaggle/input/cicids2017/cicids2017.csv\"\n",
    "    output_dir: str = \"/kaggle/working/\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    epochs: int = 25\n",
    "    batch_size: int = 512 if MULTI_GPU else 256\n",
    "    val_batch_size: int = 1024 if MULTI_GPU else 512\n",
    "    lr: float = 1.5e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    label_smoothing: float = 0.05\n",
    "    conv_channels: int = 96\n",
    "    num_layers: int = 3\n",
    "    num_heads: int = 8\n",
    "    d_model: int = 192\n",
    "    d_ff: int = 768\n",
    "    dropout: float = 0.2\n",
    "    undersampling_ratio: float = 0.15\n",
    "    ig_steps: int = 32\n",
    "    ig_samples: int = 512\n",
    "\n",
    "class CNNTokenizer(nn.Module):\n",
    "    def __init__(self, input_dim: int, conv_channels: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.proj = nn.Linear(conv_channels, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.unsqueeze(1)  # [B, 1, F]\n",
    "        tokens = self.conv(x).transpose(1, 2)  # [B, F, conv_channels]\n",
    "        tokens = self.proj(tokens)\n",
    "        return self.norm(tokens)\n",
    "\n",
    "class CNNTransformerIDS(nn.Module):\n",
    "    def __init__(self, input_dim: int, config: CNNTransformerConfig):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CNNTokenizer(input_dim, config.conv_channels, config.d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.d_ff,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.d_model))\n",
    "        self.positional = nn.Parameter(torch.randn(1, input_dim + 1, config.d_model))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(config.d_model),\n",
    "            nn.Linear(config.d_model, config.d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_model // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = self.tokenizer(x)\n",
    "        batch_size, seq_len, _ = tokens.size()\n",
    "        cls = self.cls_token.expand(batch_size, -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "        tokens = tokens + self.positional[:, :seq_len + 1]\n",
    "        encoded = self.encoder(self.dropout(tokens))\n",
    "        logits = self.classifier(encoded[:, 0])\n",
    "        return logits\n",
    "\n",
    "def detect_label_column(df: pd.DataFrame) -> str:\n",
    "    for col in df.columns:\n",
    "        if 'label' in col.lower():\n",
    "            return col\n",
    "    raise ValueError(\"No label column detected in dataset.\")\n",
    "\n",
    "def prepare_raw_features(df: pd.DataFrame, label_col: str) -> Tuple[pd.DataFrame, np.ndarray, list]:\n",
    "    df = df.copy()\n",
    "    df['binary_label'] = (df[label_col] != 'BENIGN').astype(int)\n",
    "    feature_blacklist = {label_col, 'binary_label', 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp'}\n",
    "    feature_cols = [c for c in df.columns if c not in feature_blacklist]\n",
    "    X = df[feature_cols].copy()\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    return X, df['binary_label'].values, feature_cols\n",
    "\n",
    "def split_scale_data(X: pd.DataFrame, y: np.ndarray, config: CNNTransformerConfig):\n",
    "    X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=config.test_size, stratify=y, random_state=config.random_state\n",
    "    )\n",
    "    train_medians = X_train_raw.median()\n",
    "    X_train_raw = X_train_raw.fillna(train_medians)\n",
    "    X_val_raw = X_val_raw.fillna(train_medians)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train_raw)\n",
    "    X_val = scaler.transform(X_val_raw)\n",
    "    return X_train, X_val, y_train, y_val, scaler, train_medians\n",
    "\n",
    "def build_loaders(X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, config: CNNTransformerConfig):\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "    class_counts = np.bincount(y_train)\n",
    "    weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = weights[y_train]\n",
    "    sampler = WeightedRandomSampler(torch.DoubleTensor(sample_weights), len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader, val_dataset\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_data, batch_target in loader:\n",
    "        batch_data = batch_data.to(DEVICE, non_blocking=True)\n",
    "        batch_target = batch_target.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(batch_data)\n",
    "        loss = criterion(logits, batch_target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / max(len(loader), 1)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_probs, all_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_target in loader:\n",
    "            batch_data = batch_data.to(DEVICE, non_blocking=True)\n",
    "            batch_target = batch_target.to(DEVICE, non_blocking=True)\n",
    "            logits = model(batch_data)\n",
    "            loss = criterion(logits, batch_target)\n",
    "            losses.append(loss.item())\n",
    "            probs = F.softmax(logits, dim=1)[:, 1]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_targets.extend(batch_target.cpu().numpy())\n",
    "    metrics = calculate_comprehensive_metrics(\n",
    "        np.array(all_targets), np.array(all_preds), np.array(all_probs)\n",
    "    )\n",
    "    return np.mean(losses) if losses else 0.0, metrics, np.array(all_probs), np.array(all_targets)\n",
    "\n",
    "def integrated_gradients(model, inputs, baseline=None, steps=32, target_class=1):\n",
    "    model.eval()\n",
    "    device = inputs.device\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(inputs, device=device)\n",
    "    total_gradients = torch.zeros_like(inputs)\n",
    "    for alpha in torch.linspace(0, 1, steps, device=device):\n",
    "        interpolated = baseline + alpha * (inputs - baseline)\n",
    "        interpolated.requires_grad_(True)\n",
    "        outputs = model(interpolated)\n",
    "        target = outputs[:, target_class].sum()\n",
    "        grads = torch.autograd.grad(target, interpolated, retain_graph=False)[0]\n",
    "        total_gradients += grads\n",
    "    avg_gradients = total_gradients / steps\n",
    "    return (inputs - baseline) * avg_gradients\n",
    "\n",
    "def generate_ig_report(model, X_val: np.ndarray, feature_names: list, config: CNNTransformerConfig) -> str:\n",
    "    sample_count = min(config.ig_samples, X_val.shape[0])\n",
    "    if sample_count == 0:\n",
    "        return \"\"\n",
    "    sample_idx = np.random.RandomState(config.random_state).choice(X_val.shape[0], sample_count, replace=False)\n",
    "    data = torch.FloatTensor(X_val[sample_idx]).to(DEVICE)\n",
    "    baseline_vector = torch.FloatTensor(X_val.mean(axis=0, keepdims=True)).to(DEVICE)\n",
    "    ig_values = []\n",
    "    for chunk in torch.split(data, 128):\n",
    "        base_chunk = baseline_vector.expand(chunk.size(0), -1)\n",
    "        ig_chunk = integrated_gradients(model, chunk, baseline=base_chunk, steps=config.ig_steps)\n",
    "        ig_values.append(ig_chunk.detach().cpu())\n",
    "    ig_tensor = torch.cat(ig_values, dim=0)\n",
    "    importance = ig_tensor.abs().mean(dim=0).numpy()\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'avg_abs_integrated_grad': importance\n",
    "    }).sort_values('avg_abs_integrated_grad', ascending=False)\n",
    "    csv_path = os.path.join(config.output_dir, 'cnn_transformer_integrated_gradients.csv')\n",
    "    importance_df.to_csv(csv_path, index=False)\n",
    "    print(f\"üß† Saved Integrated Gradients feature ranking -> {csv_path}\")\n",
    "    return csv_path\n",
    "\n",
    "def train_cnn_transformer(run_training: bool = True):\n",
    "    if not run_training:\n",
    "        print(\"Set RUN_CNN_TRANSFORMER=True to launch training.\")\n",
    "        return\n",
    "    config = CNNTransformerConfig()\n",
    "    torch.manual_seed(config.random_state)\n",
    "    np.random.seed(config.random_state)\n",
    "    random.seed(config.random_state)\n",
    "    print(\"üöÄ Starting CNN-Transformer IDS training\")\n",
    "    df = pd.read_csv(config.input_path)\n",
    "    label_col = detect_label_column(df)\n",
    "    X, y, feature_cols = prepare_raw_features(df, label_col)\n",
    "    X_train, X_val, y_train, y_val, scaler, medians = split_scale_data(X, y, config)\n",
    "    balancer = IntelligentDataBalancer(config.undersampling_ratio, config.random_state)\n",
    "    X_train_bal, y_train_bal = balancer.balance_classes(X_train, y_train)\n",
    "    train_loader, val_loader, val_dataset = build_loaders(X_train_bal, y_train_bal, X_val, y_val, config)\n",
    "    model = CNNTransformerIDS(input_dim=X_train.shape[1], config=config).to(DEVICE)\n",
    "    if MULTI_GPU:\n",
    "        model = DataParallel(model)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config.lr, epochs=config.epochs, steps_per_epoch=len(train_loader)\n",
    "    ) if len(train_loader) > 0 else None\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "        val_loss, metrics, _, _ = evaluate_epoch(model, val_loader, criterion)\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f} | \"\n",
    "            f\"ROC-AUC {metrics['auc_roc']:.4f} | F1 {metrics['f1_score']:.4f}\"\n",
    "        )\n",
    "        if metrics['auc_roc'] > best_auc:\n",
    "            best_auc = metrics['auc_roc']\n",
    "            best_state = {\n",
    "                'model_state_dict': model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'config': config.__dict__,\n",
    "                'feature_columns': feature_cols\n",
    "            }\n",
    "    if best_state is None:\n",
    "        print(\"Training did not improve beyond initialization; aborting save.\")\n",
    "        return\n",
    "    model_path = os.path.join(config.output_dir, 'cnn_transformer_ids.pth')\n",
    "    torch.save(best_state, model_path)\n",
    "    print(f\"üíæ Saved best CNN-Transformer checkpoint -> {model_path}\")\n",
    "\n",
    "    preprocess_artifacts = {\n",
    "        'feature_columns': feature_cols,\n",
    "        'medians': medians.to_dict(),\n",
    "        'scaler_mean': scaler.mean_.tolist(),\n",
    "        'scaler_scale': scaler.scale_.tolist()\n",
    "    }\n",
    "    preprocess_path = os.path.join(config.output_dir, 'cnn_transformer_preprocess.pkl')\n",
    "    joblib.dump(preprocess_artifacts, preprocess_path)\n",
    "    print(f\"üíæ Saved preprocessing artifacts -> {preprocess_path}\")\n",
    "\n",
    "    # Load best weights for interpretability\n",
    "    if isinstance(model, DataParallel):\n",
    "        model.module.load_state_dict(best_state['model_state_dict'])\n",
    "        final_model = model.module\n",
    "    else:\n",
    "        model.load_state_dict(best_state['model_state_dict'])\n",
    "        final_model = model\n",
    "    generate_ig_report(final_model, X_val, feature_cols, config)\n",
    "\n",
    "if __name__ == '__main__' and RUN_CNN_TRANSFORMER:\n",
    "    train_cnn_transformer(run_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T19:16:50.949301Z",
     "iopub.status.busy": "2025-08-30T19:16:50.948715Z",
     "iopub.status.idle": "2025-08-30T19:23:26.202349Z",
     "shell.execute_reply": "2025-08-30T19:23:26.201794Z",
     "shell.execute_reply.started": "2025-08-30T19:16:50.949277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SHAP config -> CHUNK_SIZE=256, BG_SIZE=2000, EVAL_SIZE=2000\n",
      "üíª Using device: cuda:0\n",
      "   GPU: Tesla T4 | Mem: 14.7 GB\n",
      "üì¶ Loading checkpoint (full pickle, weights_only=False)‚Ä¶\n",
      "‚úÖ Loaded with weights_only=False\n",
      "‚úÖ Model reconstructed & weights loaded\n",
      "‚úÖ Preprocessor restored from checkpoint\n",
      "üìÑ Reading headers to derive feature columns‚Ä¶\n",
      "‚úÖ Found 78 raw feature columns\n",
      "üì• Building evaluation slice‚Ä¶\n",
      "‚úÖ Applied restored preprocessor\n",
      "‚úÖ Eval pool ready in 41.9s (pool=150,000)\n",
      "üßÆ Building SHAP Explainer‚Ä¶\n",
      "‚úÖ DeepExplainer ready in 0.0s (bg=2000, eval=2000)\n",
      "üèÅ Starting SHAP computation‚Ä¶\n",
      "   ‚Üí SHAP 256/2000 (12%)\n",
      "   ‚Üí SHAP 512/2000 (25%)\n",
      "   ‚Üí SHAP 768/2000 (38%)\n",
      "   ‚Üí SHAP 1024/2000 (51%)\n",
      "   ‚Üí SHAP 1280/2000 (64%)\n",
      "   ‚Üí SHAP 1536/2000 (76%)\n",
      "   ‚Üí SHAP 1792/2000 (89%)\n",
      "   ‚Üí SHAP 2000/2000 (100%)\n",
      "‚úÖ SHAP complete in 351.1s\n",
      "‚úÖ SHAP values ready (no-additivity mode)\n",
      "üíæ Saved global SHAP importances -> /kaggle/working/shap_global_importance_attack.csv\n",
      "üñºÔ∏è Saved summary plot -> /kaggle/working/shap_summary_attack.png\n",
      "üñºÔ∏è Saved waterfall plot -> /kaggle/working/shap_waterfall_attack.png\n",
      "\n",
      "üèÜ Top features by mean |SHAP| (Attack):\n",
      " 1.                 PSH Flag Count  1.842313\n",
      " 2.                 ACK Flag Count  0.802594\n",
      " 3.                      Idle Mean  0.615738\n",
      " 4.                     Active Min  0.470652\n",
      " 5.            Subflow Fwd Packets  0.415745\n",
      " 6.                    Fwd IAT Std  0.410397\n",
      " 7.                  Down/Up Ratio  0.367021\n",
      " 8.          Fwd Packet Length Max  0.337219\n",
      " 9.                     Active Std  0.336550\n",
      "10.        Init_Win_bytes_backward  0.333903\n",
      "11.                       Idle Min  0.287615\n",
      "12.               act_data_pkt_fwd  0.244901\n",
      "13.          Bwd Packet Length Std  0.212523\n",
      "14.                    Bwd IAT Std  0.199012\n",
      "15.                       Idle Std  0.198254\n",
      "16.               Destination Port  0.184043\n",
      "17.           min_seg_size_forward  0.177685\n",
      "18.              Max Packet Length  0.170190\n",
      "19.                  Fwd IAT Total  0.162992\n",
      "20.              Min Packet Length  0.159424\n",
      "\n",
      "üéâ Done.\n",
      "   ‚Ä¢ Global CSV: /kaggle/working/shap_global_importance_attack.csv\n",
      "   ‚Ä¢ Summary PNG: /kaggle/working/shap_summary_attack.png\n",
      "   ‚Ä¢ Waterfall PNG: /kaggle/working/shap_waterfall_attack.png\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# SHAP on downloaded RT-IDS checkpoint (no training code needed)\n",
    "# - Robust checkpoint loader (PyTorch 2.6 safe_globals)\n",
    "# - Exact class/attribute names matching your saved state_dict\n",
    "# - Restores saved preprocessor if present (else Quantile fallback)\n",
    "# - SHAP DeepExplainer with progress + additivity disabled\n",
    "# - Saves CSV + plots; prints frequent progress updates\n",
    "# ===============================================================\n",
    "\n",
    "# (Optional) ensure shap installed; comment out if your env already has it\n",
    "# !pip install shap -q\n",
    "\n",
    "import os, gc, time, math, warnings, sys, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "CKPT_PATH  = \"/kaggle/input/v1/pytorch/default/1/enhanced_binary_rtids_model_no_synthetic_no_features_no_early_stopping.pth\"\n",
    "DATA_PATH  = \"/kaggle/input/cicids2017/cicids2017.csv\"  # change if your file is elsewhere\n",
    "OUT_DIR    = \"/kaggle/working\"\n",
    "CHUNK_SIZE = 256     # SHAP batch per iteration (progress-printed)\n",
    "BG_SIZE    = 2000    # background samples for DeepExplainer\n",
    "EVAL_SIZE  = 2000    # number of samples to explain\n",
    "EVAL_POOL  = 150_000 # build eval pool from this many random rows (processing once)\n",
    "RANDOM_SEED= 42\n",
    "PLOT_TOPK  = 20\n",
    "\n",
    "# -------------------- LOGGING -------------------\n",
    "def log(msg): \n",
    "    print(msg, flush=True)\n",
    "\n",
    "def secs(t): \n",
    "    return f\"{t:.1f}s\"\n",
    "\n",
    "log(f\"üîß SHAP config -> CHUNK_SIZE={CHUNK_SIZE}, BG_SIZE={BG_SIZE}, EVAL_SIZE={EVAL_SIZE}\")\n",
    "\n",
    "# -------------------- DEVICE --------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    log(\"üíª Using device: cuda:0\")\n",
    "    try:\n",
    "        log(f\"   GPU: {torch.cuda.get_device_name(0)} | Mem: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    log(\"üíª Using device: cpu\")\n",
    "\n",
    "# -------------------- DUMMY CLASSES (pickle safety) --------------------\n",
    "# These satisfy objects saved inside the checkpoint (e.g., preprocessor)\n",
    "from sklearn.preprocessing import QuantileTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "class RobustPreprocessor:\n",
    "    def __init__(self, scaling_method='quantile', handle_outliers=True, n_features=120):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.n_features = n_features\n",
    "        self.scaler = None\n",
    "        self.feature_selector = None\n",
    "    def fit_transform(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Not used in inference.\")\n",
    "    def transform(self, X):\n",
    "        Xp = X.replace([np.inf, -np.inf], np.nan)\n",
    "        for c in Xp.columns:\n",
    "            if Xp[c].isna().sum() > 0:\n",
    "                Xp[c].fillna(Xp[c].median(), inplace=True)\n",
    "        if self.feature_selector is not None:\n",
    "            try:\n",
    "                Xp = self.feature_selector.transform(Xp)\n",
    "                Xp = pd.DataFrame(Xp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if self.scaler is not None:\n",
    "            try:\n",
    "                return self.scaler.transform(Xp)\n",
    "            except Exception:\n",
    "                return Xp.values\n",
    "        return Xp.values\n",
    "\n",
    "class IntelligentDataBalancer:\n",
    "    def __init__(self, undersampling_ratio=0.12, random_state=42):\n",
    "        self.undersampling_ratio = undersampling_ratio\n",
    "        self.random_state = random_state\n",
    "    def balance_classes(self, X, y):\n",
    "        return X, y\n",
    "\n",
    "# -------------------- ARCHITECTURE (names match checkpoint) --------------------\n",
    "class FeatureImportanceLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.projection = nn.Linear(input_dim, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        imp = self.feature_attention(x)\n",
    "        emb = self.projection(x * imp)\n",
    "        emb = self.layer_norm(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        return emb.unsqueeze(1), imp\n",
    "\n",
    "class EnhancedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv_projection = nn.Linear(d_model, d_model*3, bias=False)\n",
    "        self.output_projection = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    def forward(self, x):\n",
    "        B,T,D = x.size()\n",
    "        qkv = self.qkv_projection(x).reshape(B,T,3,self.num_heads,self.d_k).permute(2,0,3,1,4)\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, v).transpose(1,2).contiguous().reshape(B,T,D)\n",
    "        return self.output_projection(out)\n",
    "\n",
    "class EnhancedTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = EnhancedMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual_scale = nn.Parameter(torch.ones(1)*0.8)\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x)))   * self.residual_scale\n",
    "        x = x + self.dropout(self.feed_forward(self.norm2(x))) * self.residual_scale\n",
    "        return x\n",
    "\n",
    "class AttentionPoolingClassifier(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.attention_pool = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,d_model))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, d_model//4), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//4, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        cls = self.cls_token.expand(B,-1,-1)\n",
    "        x_with_cls = torch.cat([cls, x], dim=1)\n",
    "        pooled,_ = self.attention_pool(cls, x_with_cls, x_with_cls)\n",
    "        return self.classifier(pooled.squeeze(1))\n",
    "\n",
    "class EnhancedBinaryTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=160, num_layers=4, num_heads=10, d_ff=640, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.feature_embedder = FeatureImportanceLayer(input_dim, d_model)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            EnhancedTransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = AttentionPoolingClassifier(d_model, dropout)\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0); nn.init.constant_(m.weight, 1.0)\n",
    "    def forward(self, x):\n",
    "        x, feat_imp = self.feature_embedder(x)\n",
    "        for blk in self.transformer_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits, feat_imp\n",
    "\n",
    "# -------------------- LOAD CHECKPOINT ROBUSTLY --------------------\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at: {CKPT_PATH}\")\n",
    "\n",
    "log(\"üì¶ Loading checkpoint (full pickle, weights_only=False)‚Ä¶\")\n",
    "try:\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "    log(\"‚úÖ Loaded with weights_only=False\")\n",
    "except Exception as e:\n",
    "    log(f\"‚ö†Ô∏è Full load failed: {e}\")\n",
    "    # expand allowed globals for PyTorch 2.6 safe unpickler\n",
    "    try:\n",
    "        from torch.serialization import add_safe_globals\n",
    "        import numpy as _np\n",
    "        add_safe_globals([\n",
    "            _np.dtype, _np.float64, _np.int64, _np.float32, _np.int32,\n",
    "            _np.dtypes.Float64DType, _np.dtypes.Int64DType, _np.core.multiarray.scalar\n",
    "        ])\n",
    "        ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "        log(\"‚úÖ Loaded after allowlisting numpy dtypes\")\n",
    "    except Exception as e2:\n",
    "        log(f\"‚ùå Still failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "state = ckpt[\"model_state_dict\"]\n",
    "cfg   = ckpt.get(\"config\", {})\n",
    "def _cfg(k, dv): return cfg.get(k, dv)\n",
    "\n",
    "# infer input_dim from weight shape of first linear in embedder\n",
    "input_dim = state[\"feature_embedder.projection.weight\"].shape[1]\n",
    "\n",
    "model = EnhancedBinaryTransformerClassifier(\n",
    "    input_dim=input_dim,\n",
    "    d_model=_cfg(\"d_model\",160),\n",
    "    num_layers=_cfg(\"num_layers\",4),\n",
    "    num_heads=_cfg(\"heads\",10),\n",
    "    d_ff=_cfg(\"d_ff\",640),\n",
    "    dropout=_cfg(\"dropout\",0.15)\n",
    ")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model = model.to(device).eval()\n",
    "log(\"‚úÖ Model reconstructed & weights loaded\")\n",
    "\n",
    "preprocessor = ckpt.get(\"preprocessor\", None)\n",
    "if preprocessor is not None:\n",
    "    log(\"‚úÖ Preprocessor restored from checkpoint\")\n",
    "else:\n",
    "    log(\"‚ö†Ô∏è Preprocessor missing; will use QuantileTransformer fallback\")\n",
    "\n",
    "# -------------------- FEATURES / COLUMNS --------------------\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"DATA_PATH not found: {DATA_PATH}\")\n",
    "\n",
    "log(\"üìÑ Reading headers to derive feature columns‚Ä¶\")\n",
    "df_head = pd.read_csv(DATA_PATH, nrows=5)\n",
    "label_candidates = [c for c in df_head.columns if 'label' in c.lower()]\n",
    "if not label_candidates:\n",
    "    raise RuntimeError(\"Could not find a label column in the dataset headers.\")\n",
    "label_col = label_candidates[0]\n",
    "exclude_cols = [label_col, 'binary_label', 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "feature_cols = [c for c in df_head.columns if c not in exclude_cols]\n",
    "log(f\"‚úÖ Found {len(feature_cols)} raw feature columns\")\n",
    "\n",
    "# If checkpoint's selector exists, get selected feature names in order\n",
    "if preprocessor is not None and getattr(preprocessor, \"feature_selector\", None) is not None:\n",
    "    try:\n",
    "        sel_idx = preprocessor.feature_selector.get_support(indices=True)\n",
    "        selected_feature_names = [feature_cols[i] for i in sel_idx]\n",
    "    except Exception:\n",
    "        selected_feature_names = feature_cols\n",
    "else:\n",
    "    selected_feature_names = feature_cols\n",
    "\n",
    "# -------------------- BUILD EVAL POOL & PREPROCESS --------------------\n",
    "np.random.seed(RANDOM_SEED)\n",
    "log(\"üì• Building evaluation slice‚Ä¶\")\n",
    "t0 = time.time()\n",
    "\n",
    "usecols = list(dict.fromkeys(feature_cols + [label_col]))  # preserve order\n",
    "full = pd.read_csv(DATA_PATH, usecols=usecols)\n",
    "full[\"binary_label\"] = (full[label_col] != \"BENIGN\").astype(int)\n",
    "\n",
    "X_raw = full[feature_cols].copy()\n",
    "# cast non-numeric\n",
    "for c in X_raw.select_dtypes(include=[\"object\"]).columns:\n",
    "    X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "X_raw = X_raw.replace([np.inf, -np.inf], np.nan).fillna(X_raw.median())\n",
    "\n",
    "# apply saved preprocessor or fallback\n",
    "if preprocessor is not None:\n",
    "    try:\n",
    "        X_proc = preprocessor.transform(X_raw)\n",
    "        log(\"‚úÖ Applied restored preprocessor\")\n",
    "    except Exception as e:\n",
    "        log(f\"‚ö†Ô∏è preprocessor.transform failed ({e}); using fallback QuantileTransformer\")\n",
    "        qt = QuantileTransformer(output_distribution=\"uniform\", random_state=42)\n",
    "        X_proc = qt.fit_transform(X_raw)\n",
    "        selected_feature_names = feature_cols\n",
    "else:\n",
    "    qt = QuantileTransformer(output_distribution=\"uniform\", random_state=42)\n",
    "    X_proc = qt.fit_transform(X_raw)\n",
    "    selected_feature_names = feature_cols\n",
    "    log(\"‚ö†Ô∏è Used fallback QuantileTransformer\")\n",
    "\n",
    "y_all = full[\"binary_label\"].values\n",
    "\n",
    "pool_n = min(EVAL_POOL, len(y_all))\n",
    "pool_idx = np.random.RandomState(RANDOM_SEED).choice(len(y_all), size=pool_n, replace=False)\n",
    "X_eval_pool = X_proc[pool_idx]\n",
    "y_eval_pool = y_all[pool_idx]\n",
    "log(f\"‚úÖ Eval pool ready in {secs(time.time()-t0)} (pool={X_eval_pool.shape[0]:,})\")\n",
    "\n",
    "# -------------------- SHAP SETUP --------------------\n",
    "class LogitsOnly(nn.Module):\n",
    "    def __init__(self, base): \n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        return out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "logits_model = LogitsOnly(model).to(device).eval()\n",
    "\n",
    "import shap\n",
    "\n",
    "# Prefer DeepExplainer; fallback to GradientExplainer if needed\n",
    "log(\"üßÆ Building SHAP Explainer‚Ä¶\")\n",
    "bg_size  = min(BG_SIZE, X_eval_pool.shape[0])\n",
    "eval_sz  = min(EVAL_SIZE, X_eval_pool.shape[0])\n",
    "\n",
    "X_t       = torch.from_numpy(X_eval_pool).float().to(device)\n",
    "bg_idx    = np.random.RandomState(7).choice(X_t.shape[0], size=bg_size, replace=False)\n",
    "background= X_t[bg_idx]\n",
    "eval_data = X_t[:eval_sz]\n",
    "\n",
    "explainer = None\n",
    "t_exp = time.time()\n",
    "try:\n",
    "    explainer = shap.DeepExplainer(logits_model, background)\n",
    "    mode = \"DeepExplainer\"\n",
    "    log(f\"‚úÖ {mode} ready in {secs(time.time()-t_exp)} (bg={bg_size}, eval={eval_sz})\")\n",
    "except Exception as e:\n",
    "    log(f\"‚ö†Ô∏è DeepExplainer failed: {e} -> falling back to GradientExplainer\")\n",
    "    try:\n",
    "        explainer = shap.GradientExplainer(logits_model, background)\n",
    "        mode = \"GradientExplainer\"\n",
    "        log(f\"‚úÖ {mode} ready in {secs(time.time()-t_exp)} (bg={bg_size}, eval={eval_sz})\")\n",
    "    except Exception as e2:\n",
    "        log(f\"‚ùå GradientExplainer also failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "# -------------------- SHAP RUN (progress + no additivity) --------------------\n",
    "def shap_values_in_chunks_no_additivity(explainer, data_t, chunk=256):\n",
    "    n = data_t.shape[0]\n",
    "    sv0_parts, sv1_parts = [], []\n",
    "    start = 0\n",
    "    last_pct = -1\n",
    "    t0 = time.time()\n",
    "    while start < n:\n",
    "        end = min(start + chunk, n)\n",
    "        pct = int(100 * end / n)\n",
    "        if pct != last_pct:\n",
    "            log(f\"   ‚Üí SHAP {end}/{n} ({pct}%)\")\n",
    "            last_pct = pct\n",
    "        # Disable additivity check to avoid LayerNorm/MHA assertion\n",
    "        sv = explainer.shap_values(data_t[start:end], check_additivity=False) \\\n",
    "             if mode == \"DeepExplainer\" else explainer.shap_values(data_t[start:end].detach().cpu().numpy())\n",
    "        if isinstance(sv, (list, tuple)) and len(sv) >= 2:\n",
    "            sv0_parts.append(sv[0])\n",
    "            sv1_parts.append(sv[1])\n",
    "        else:\n",
    "            # single output fallback: duplicate to keep downstream shape\n",
    "            sv0_parts.append(sv)\n",
    "            sv1_parts.append(sv)\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        start = end\n",
    "    log(f\"‚úÖ SHAP complete in {secs(time.time()-t0)}\")\n",
    "    sv0 = np.concatenate(sv0_parts, axis=0)\n",
    "    sv1 = np.concatenate(sv1_parts, axis=0)\n",
    "    return sv0, sv1\n",
    "\n",
    "log(\"üèÅ Starting SHAP computation‚Ä¶\")\n",
    "sv0, sv1 = shap_values_in_chunks_no_additivity(explainer, eval_data, chunk=CHUNK_SIZE)\n",
    "log(\"‚úÖ SHAP values ready (no-additivity mode)\")\n",
    "\n",
    "# -------------------- SAVE / REPORT --------------------\n",
    "mean_abs = np.abs(sv1).mean(axis=0)\n",
    "global_importance = pd.DataFrame({\n",
    "    \"feature\": selected_feature_names,\n",
    "    \"mean_abs_shap\": mean_abs\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "csv_path = os.path.join(OUT_DIR, \"shap_global_importance_attack.csv\")\n",
    "global_importance.to_csv(csv_path, index=False)\n",
    "log(f\"üíæ Saved global SHAP importances -> {csv_path}\")\n",
    "\n",
    "# Try to render + save plots (headless safe)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    shap.summary_plot(\n",
    "        sv1, \n",
    "        features=eval_data.detach().cpu().numpy(),\n",
    "        feature_names=selected_feature_names,\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    sum_png = os.path.join(OUT_DIR, \"shap_summary_attack.png\")\n",
    "    plt.savefig(sum_png, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    log(f\"üñºÔ∏è Saved summary plot -> {sum_png}\")\n",
    "except Exception as e:\n",
    "    log(f\"‚ö†Ô∏è summary_plot skipped: {e}\")\n",
    "\n",
    "# Pick a confident Attack sample for local explanation\n",
    "with torch.no_grad():\n",
    "    probs = F.softmax(logits_model(eval_data), dim=1)[:,1].detach().cpu().numpy()\n",
    "\n",
    "attack_idx = np.where(y_eval_pool[:eval_sz]==1)[0]\n",
    "i_local = int(attack_idx[np.argmax(probs[attack_idx])]) if len(attack_idx)>0 else int(np.argmax(probs))\n",
    "\n",
    "try:\n",
    "    exp = shap.Explanation(\n",
    "        values=sv1[i_local],\n",
    "        base_values=np.array(explainer.expected_value[1]).mean() if hasattr(explainer, \"expected_value\") else 0.0,\n",
    "        data=eval_data[i_local].detach().cpu().numpy(),\n",
    "        feature_names=selected_feature_names\n",
    "    )\n",
    "    shap.plots.waterfall(exp, max_display=PLOT_TOPK, show=False)\n",
    "    plt.tight_layout()\n",
    "    wf_png = os.path.join(OUT_DIR, \"shap_waterfall_attack.png\")\n",
    "    plt.savefig(wf_png, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    log(f\"üñºÔ∏è Saved waterfall plot -> {wf_png}\")\n",
    "except Exception as e:\n",
    "    log(f\"‚ö†Ô∏è waterfall plot skipped: {e}\")\n",
    "\n",
    "# Small text report top features\n",
    "TOPK = 20\n",
    "log(\"\\nüèÜ Top features by mean |SHAP| (Attack):\")\n",
    "for i,(f,v) in enumerate(global_importance.head(TOPK).values, 1):\n",
    "    log(f\"{i:>2}. {f:>30}  {v:.6f}\")\n",
    "\n",
    "log(\"\\nüéâ Done.\\n\"\n",
    "    f\"   ‚Ä¢ Global CSV: {csv_path}\\n\"\n",
    "    f\"   ‚Ä¢ Summary PNG: {os.path.join(OUT_DIR, 'shap_summary_attack.png')}\\n\"\n",
    "    f\"   ‚Ä¢ Waterfall PNG: {os.path.join(OUT_DIR, 'shap_waterfall_attack.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-30T04:51:08.510028Z",
     "iopub.status.idle": "2025-08-30T04:51:08.510260Z",
     "shell.execute_reply": "2025-08-30T04:51:08.510161Z",
     "shell.execute_reply.started": "2025-08-30T04:51:08.510151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3674161,
     "sourceId": 6376134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7685292,
     "sourceId": 12200692,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 439155,
     "modelInstanceId": 421542,
     "sourceId": 552712,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2293.025374,
   "end_time": "2025-07-13T17:50:29.47529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-13T17:12:16.449916",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
