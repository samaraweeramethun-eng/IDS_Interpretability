{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98b23ea",
   "metadata": {},
   "source": [
    "# Methun Research â€” CNN-Transformer IDS with XAI\n",
    "\n",
    "This notebook runs the **full pipeline** on Google Colab:\n",
    "\n",
    "| Step | What happens |\n",
    "|------|-------------|\n",
    "| **1. Setup** | Clone repo, install deps, verify GPU |\n",
    "| **2. Data** | Upload / mount CICIDS2017 dataset |\n",
    "| **3. CNN-Transformer Training** | Train the hybrid model |\n",
    "| **4. Enhanced Transformer Training** | Train the grouped-token model |\n",
    "| **5. Integrated Gradients** | Per-feature attribution (already runs during training) |\n",
    "| **6. Grad-CAM** | CNN activation-map attribution (already runs during training) |\n",
    "| **7. SHAP** | GradientExplainer feature importance + plots |\n",
    "| **8. Test Evaluation** | Held-out test set metrics (20 % unseen data) |\n",
    "| **9. Results** | Display all XAI outputs side by side |\n",
    "\n",
    "> **Runtime:** Select **GPU â†’ T4** via *Runtime â†’ Change runtime type* before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db312af2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup â€” Clone & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/samaraweeramethun-eng/IDS_Interpretability.git\n",
    "%cd IDS_Interpretability\n",
    "\n",
    "# Install CUDA-enabled PyTorch (matches Colab T4 drivers)\n",
    "!pip install -q torch --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install project dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Explicit editable install (ensures methun_research is importable)\n",
    "!pip install -q -e .\n",
    "\n",
    "# Fallback: add src/ to sys.path in case editable install didn't register\n",
    "SRC_DIR = os.path.join(os.getcwd(), 'src')\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "# Verify the package is importable\n",
    "import methun_research\n",
    "print(f\"âœ“ methun_research imported from: {methun_research.__file__}\")\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    free, total = torch.cuda.mem_get_info(0)\n",
    "    print(f\"VRAM: {total/1024**3:.1f} GB total, {free/1024**3:.1f} GB free\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected â€” training will be slow. Select GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9ea9f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data â€” Load CICIDS2017\n",
    "\n",
    "Choose **one** of the options below:\n",
    "\n",
    "- **Option A (recommended):** Mount Google Drive if you have the full dataset there  \n",
    "- **Option B:** Upload a CSV directly from your local machine  \n",
    "- **Option C:** Use the included 5 000-row sample for a quick smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# â”€â”€ Option A: Mount Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uncomment the lines below if your dataset is on Google Drive.\n",
    "# Update the path to match where you stored the CSV.\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# DRIVE_CSV = '/content/drive/MyDrive/cicids2017/cicids2017.csv'\n",
    "# !mkdir -p data/cicids2017\n",
    "# !ln -sf \"$DRIVE_CSV\" data/cicids2017/cicids2017.csv\n",
    "\n",
    "# â”€â”€ Option B: Upload from local machine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uncomment the lines below to upload interactively.\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # select your cicids2017.csv\n",
    "# !mkdir -p data/cicids2017\n",
    "# !mv \"{list(uploaded.keys())[0]}\" data/cicids2017/cicids2017.csv\n",
    "\n",
    "# â”€â”€ Option C: Use included sample (default) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_PATH = 'data/cicids2017/cicids2017_sample.csv'\n",
    "\n",
    "# Switch to full dataset if it exists\n",
    "if os.path.exists('data/cicids2017/cicids2017.csv'):\n",
    "    DATA_PATH = 'data/cicids2017/cicids2017.csv'\n",
    "\n",
    "print(f'Using dataset: {DATA_PATH}')\n",
    "print(f'Size: {os.path.getsize(DATA_PATH) / 1024**2:.1f} MB')\n",
    "\n",
    "import pandas as pd\n",
    "df_peek = pd.read_csv(DATA_PATH, nrows=5)\n",
    "print(f'Columns: {len(df_peek.columns)}')\n",
    "label_col = [c for c in df_peek.columns if 'label' in c.lower()][0]\n",
    "counts = pd.read_csv(DATA_PATH, usecols=[label_col])[label_col].value_counts()\n",
    "print(f'\\nLabel distribution:\\n{counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f43e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Configure Training\n",
    "\n",
    "Adjust hyperparameters below. The defaults are tuned for a **T4 GPU** with the full dataset.  \n",
    "If you are using the 5 000-row sample, the `_sample` overrides will automatically apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure methun_research is importable (survives kernel restart)\n",
    "SRC_DIR = os.path.join(os.getcwd(), 'src')\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "from methun_research.config import CNNTransformerConfig, EnhancedConfig\n",
    "\n",
    "USE_SAMPLE = 'sample' in DATA_PATH\n",
    "\n",
    "# â”€â”€ CNN-Transformer Config (optimised for T4 GPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cnn_cfg = CNNTransformerConfig(\n",
    "    input_path=DATA_PATH,\n",
    "    output_dir='artifacts',\n",
    "    epochs=25 if not USE_SAMPLE else 5,\n",
    "    batch_size=1024 if not USE_SAMPLE else 64,\n",
    "    val_batch_size=2048 if not USE_SAMPLE else 128,\n",
    "    lr=2e-3 if not USE_SAMPLE else 1.5e-3,\n",
    "    num_workers=2,\n",
    "    d_model=192 if not USE_SAMPLE else 64,\n",
    "    conv_channels=96 if not USE_SAMPLE else 32,\n",
    "    num_layers=3 if not USE_SAMPLE else 1,\n",
    "    num_heads=8 if not USE_SAMPLE else 4,\n",
    "    d_ff=768 if not USE_SAMPLE else 256,\n",
    "    dropout=0.2,\n",
    "    val_size=0.1,\n",
    "    ig_steps=32 if not USE_SAMPLE else 8,\n",
    "    ig_samples=512 if not USE_SAMPLE else 128,\n",
    "    max_train_samples=500_000 if not USE_SAMPLE else 0,\n",
    ")\n",
    "\n",
    "# â”€â”€ Enhanced Transformer Config (optimised for T4 GPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "enh_cfg = EnhancedConfig(\n",
    "    input_path=DATA_PATH,\n",
    "    output_dir='artifacts',\n",
    "    epochs=35 if not USE_SAMPLE else 5,\n",
    "    batch_size=1024 if not USE_SAMPLE else 64,\n",
    "    val_batch_size=2048 if not USE_SAMPLE else 128,\n",
    "    lr=2e-3,\n",
    "    num_workers=2,\n",
    "    d_model=160 if not USE_SAMPLE else 64,\n",
    "    num_layers=4 if not USE_SAMPLE else 2,\n",
    "    heads=10 if not USE_SAMPLE else 4,\n",
    "    d_ff=640 if not USE_SAMPLE else 256,\n",
    "    dropout=0.15,\n",
    "    group_size=8,\n",
    "    use_swa=True if not USE_SAMPLE else False,\n",
    "    use_mixup=True if not USE_SAMPLE else False,\n",
    "    max_train_samples=500_000 if not USE_SAMPLE else 0,\n",
    ")\n",
    "\n",
    "print(f'Mode: {\"SAMPLE (small model)\" if USE_SAMPLE else \"FULL DATASET (production model)\"}')\n",
    "print(f'\\nData split: {100*(1-cnn_cfg.val_size-cnn_cfg.test_size):.0f}% train / {100*cnn_cfg.val_size:.0f}% val / {100*cnn_cfg.test_size:.0f}% test')\n",
    "print(f'\\nCNN-Transformer: {cnn_cfg.epochs} epochs, d_model={cnn_cfg.d_model}, batch={cnn_cfg.batch_size}')\n",
    "print(f'Enhanced:        {enh_cfg.epochs} epochs, d_model={enh_cfg.d_model}, batch={enh_cfg.batch_size}')\n",
    "print(f'Max train samples: {cnn_cfg.max_train_samples or \"unlimited\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb7350",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train CNN-Transformer\n",
    "\n",
    "This trains the CNN + Transformer hybrid and automatically generates:\n",
    "- **Integrated Gradients** feature attributions\n",
    "- **Grad-CAM** activation maps over feature positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time, torch\n",
    "from methun_research.training.cnn_trainer import train_cnn_transformer\n",
    "\n",
    "# Show RAM before training\n",
    "import psutil\n",
    "ram = psutil.virtual_memory()\n",
    "print(f'System RAM: {ram.used/1024**3:.1f} / {ram.total/1024**3:.1f} GB ({ram.percent}% used)')\n",
    "if torch.cuda.is_available():\n",
    "    free, total = torch.cuda.mem_get_info(0)\n",
    "    print(f'GPU VRAM:   {(total-free)/1024**3:.1f} / {total/1024**3:.1f} GB used')\n",
    "\n",
    "gc.collect()\n",
    "print('\\nTraining CNN-Transformer...')\n",
    "t0 = time.time()\n",
    "cnn_path = train_cnn_transformer(cnn_cfg)\n",
    "\n",
    "elapsed = (time.time() - t0) / 60\n",
    "print(f'\\nDone in {elapsed:.1f} min  ->  {cnn_path}')\n",
    "\n",
    "# Show resource usage after training\n",
    "ram = psutil.virtual_memory()\n",
    "print(f'\\nSystem RAM: {ram.used/1024**3:.1f} / {ram.total/1024**3:.1f} GB ({ram.percent}% used)')\n",
    "if torch.cuda.is_available():\n",
    "    free, total = torch.cuda.mem_get_info(0)\n",
    "    print(f'GPU VRAM:   {(total-free)/1024**3:.1f} / {total/1024**3:.1f} GB used')\n",
    "    print(f'GPU peak:   {torch.cuda.max_memory_allocated(0)/1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf657e",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train Enhanced Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ccdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from methun_research.training.enhanced_trainer import train_enhanced\n",
    "\n",
    "print('Training Enhanced Transformer...')\n",
    "t0 = time.time()\n",
    "enh_path = train_enhanced(enh_cfg)\n",
    "\n",
    "elapsed = (time.time() - t0) / 60\n",
    "print(f'\\nDone in {elapsed:.1f} min  ->  {enh_path}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    free, total = torch.cuda.mem_get_info(0)\n",
    "    print(f'GPU VRAM:   {(total-free)/1024**3:.1f} / {total/1024**3:.1f} GB used')\n",
    "    print(f'GPU peak:   {torch.cuda.max_memory_allocated(0)/1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee748358",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. SHAP Analysis â€” CNN-Transformer\n",
    "\n",
    "Runs `GradientExplainer` on the CNN-Transformer checkpoint.  \n",
    "Produces: global importance CSV, summary beeswarm plot, single-sample waterfall plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd05743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methun_research.interpretability.shap_runner import run_shap\n",
    "\n",
    "os.makedirs('artifacts/shap', exist_ok=True)\n",
    "\n",
    "SHAP_BG   = 2000 if not USE_SAMPLE else 200\n",
    "SHAP_EVAL = 2000 if not USE_SAMPLE else 200\n",
    "SHAP_POOL = 150_000 if not USE_SAMPLE else 500\n",
    "\n",
    "print('Running SHAP on CNN-Transformer...')\n",
    "t0 = time.time()\n",
    "shap_csv = run_shap(\n",
    "    checkpoint_path=cnn_path,\n",
    "    data_path=DATA_PATH,\n",
    "    output_dir='artifacts/shap',\n",
    "    background_size=SHAP_BG,\n",
    "    eval_size=SHAP_EVAL,\n",
    "    eval_pool=SHAP_POOL,\n",
    "    chunk_size=256,\n",
    ")\n",
    "print(f'\\nDone in {(time.time()-t0)/60:.1f} min  ->  {shap_csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f958",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. SHAP Analysis â€” Enhanced Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b786fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if enh_path:\n",
    "    print('Running SHAP on Enhanced Transformer...')\n",
    "    t0 = time.time()\n",
    "    shap_enh_csv = run_shap(\n",
    "        checkpoint_path=enh_path,\n",
    "        data_path=DATA_PATH,\n",
    "        output_dir='artifacts/shap_enhanced',\n",
    "        background_size=SHAP_BG,\n",
    "        eval_size=SHAP_EVAL,\n",
    "        eval_pool=SHAP_POOL,\n",
    "        chunk_size=256,\n",
    "    )\n",
    "    print(f'\\nDone in {(time.time()-t0)/60:.1f} min  ->  {shap_enh_csv}')\n",
    "else:\n",
    "    print('No Enhanced Transformer checkpoint â€” skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97e321",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Test Set Evaluation Summary\n",
    "\n",
    "Both trainers automatically evaluate on a **held-out 20% test set** that is never seen during training or validation.  \n",
    "This cell loads the saved test metrics from each checkpoint and presents them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0311950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def load_test_metrics(ckpt_path, model_name):\n",
    "    \"\"\"Load test_metrics from a saved checkpoint.\"\"\"\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f'  [{model_name}] Checkpoint not found: {ckpt_path}')\n",
    "        return None\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    if 'test_metrics' not in ckpt:\n",
    "        print(f'  [{model_name}] No test_metrics in checkpoint (was test_size=0?)')\n",
    "        return None\n",
    "    m = ckpt['test_metrics']\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'ROC-AUC': f\"{m['auc_roc']:.4f}\",\n",
    "        'PR-AUC': f\"{m['auc_pr']:.4f}\",\n",
    "        'F1-Score': f\"{m['f1_score']:.4f}\",\n",
    "        'Precision': f\"{m['precision']:.4f}\",\n",
    "        'Recall': f\"{m['recall']:.4f}\",\n",
    "        'Accuracy': f\"{m['accuracy']:.4f}\",\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "r = load_test_metrics(cnn_path, 'CNN-Transformer')\n",
    "if r: rows.append(r)\n",
    "r = load_test_metrics(enh_path, 'Enhanced Transformer')\n",
    "if r: rows.append(r)\n",
    "\n",
    "if rows:\n",
    "    df_test = pd.DataFrame(rows).set_index('Model')\n",
    "    display(HTML('<h3>ðŸ§ª Held-Out Test Set Results (20 % of data, never seen during training)</h3>'))\n",
    "    display(df_test.style.set_properties(**{'text-align': 'center'}).format(precision=4))\n",
    "else:\n",
    "    print('No test metrics available. Ensure test_size > 0 in configs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3433bae",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualise XAI Results\n",
    "\n",
    "Compare feature rankings across all four XAI methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc65d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "TOP_K = 15\n",
    "\n",
    "def load_ranking(path, val_col, method_name):\n",
    "    if not os.path.exists(path):\n",
    "        print(f'  [{method_name}] Not found: {path}')\n",
    "        return None\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(val_col, ascending=False).head(TOP_K).reset_index(drop=True)\n",
    "    df.index += 1\n",
    "    df.columns = ['Feature', method_name]\n",
    "    return df\n",
    "\n",
    "rankings = [\n",
    "    load_ranking('artifacts/cnn_transformer_integrated_gradients.csv',\n",
    "                 'avg_abs_integrated_grad', 'Integrated Gradients'),\n",
    "    load_ranking('artifacts/cnn_transformer_grad_cam.csv',\n",
    "                 'grad_cam_importance', 'Grad-CAM'),\n",
    "    load_ranking('artifacts/shap/shap_global_importance_attack.csv',\n",
    "                 'mean_abs_shap', 'SHAP (CNN-Trans)'),\n",
    "]\n",
    "\n",
    "# Enhanced SHAP if available\n",
    "r = load_ranking('artifacts/shap_enhanced/shap_global_importance_attack.csv',\n",
    "                 'mean_abs_shap', 'SHAP (Enhanced)')\n",
    "if r is not None:\n",
    "    rankings.append(r)\n",
    "\n",
    "rankings = [r for r in rankings if r is not None]\n",
    "if rankings:\n",
    "    combined = pd.concat([r.set_index(r.index) for r in rankings], axis=1)\n",
    "    display(HTML(f'<h3>Top {TOP_K} Features â€” All XAI Methods</h3>'))\n",
    "    display(combined)\n",
    "else:\n",
    "    print('No ranking files found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97613b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated plots\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plots = [\n",
    "    ('Grad-CAM Feature Importance', 'artifacts/grad_cam_importance.png'),\n",
    "    ('SHAP Summary (CNN-Transformer)', 'artifacts/shap/shap_summary_attack.png'),\n",
    "    ('SHAP Waterfall (CNN-Transformer)', 'artifacts/shap/shap_waterfall_attack.png'),\n",
    "    ('SHAP Summary (Enhanced)', 'artifacts/shap_enhanced/shap_summary_attack.png'),\n",
    "    ('SHAP Waterfall (Enhanced)', 'artifacts/shap_enhanced/shap_waterfall_attack.png'),\n",
    "]\n",
    "\n",
    "for title, path in plots:\n",
    "    if os.path.exists(path):\n",
    "        print(f'\\n--- {title} ---')\n",
    "        display(Image(filename=path, width=700))\n",
    "    else:\n",
    "        print(f'[skip] {title} â€” file not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b5f18",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Artifacts to Google Drive (Optional)\n",
    "\n",
    "Copy all checkpoints and XAI outputs to your Drive for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8439ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save artifacts to Google Drive\n",
    "# import shutil\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# DEST = '/content/drive/MyDrive/methun_research_artifacts'\n",
    "# shutil.copytree('artifacts', DEST, dirs_exist_ok=True)\n",
    "# print(f'Artifacts saved to {DEST}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f672f",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. List All Generated Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All generated artifacts:')\n",
    "for root, dirs, files in os.walk('artifacts'):\n",
    "    for f in sorted(files):\n",
    "        fp = os.path.join(root, f)\n",
    "        sz = os.path.getsize(fp)\n",
    "        if sz > 1024*1024:\n",
    "            print(f'  {fp}  ({sz/1024**2:.1f} MB)')\n",
    "        else:\n",
    "            print(f'  {fp}  ({sz/1024:.1f} KB)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
